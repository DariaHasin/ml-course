{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW: X-ray images classification\n",
    "--------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edited by Bar Goldner & Daria Hasin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you begin, open Mobaxterm and connect to triton with the user and password you were give with. Activate the environment `2ndPaper` and then type the command `pip install scikit-image`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will be dealing with classification of 32X32 X-ray images of the chest. The image can be classified into one of four options: lungs (l), clavicles (c), and heart (h) and background (b). Even though those labels are dependent, we will treat this task as multiclass and not as multilabel. The dataset for this assignment is located on a shared folder on triton (`/MLdata/MLcourse/X_ray/'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smorandv/anaconda3/envs/2ndPaper-t2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/smorandv/anaconda3/envs/2ndPaper-t2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/smorandv/anaconda3/envs/2ndPaper-t2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/smorandv/anaconda3/envs/2ndPaper-t2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/smorandv/anaconda3/envs/2ndPaper-t2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/smorandv/anaconda3/envs/2ndPaper-t2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense, MaxPool2D, Conv2D, Dropout\n",
    "from tensorflow.keras.layers import Flatten, InputLayer\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import *\n",
    "\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import *\n",
    "from skimage.io import imread\n",
    "\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.compat.v1.ConfigProto(gpu_options =\n",
    "                         tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.2)\n",
    "# device_count = {'GPU': 1}\n",
    ")\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "tf.compat.v1.keras.backend.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(datapath):\n",
    "    # This part reads the images\n",
    "    classes = ['b','c','l','h']\n",
    "    imagelist = [fn for fn in os.listdir(datapath)]\n",
    "    N = len(imagelist)\n",
    "    num_classes = len(classes)\n",
    "    images = np.zeros((N, 32, 32, 1))\n",
    "    Y = np.zeros((N,num_classes))\n",
    "    ii=0\n",
    "    for fn in imagelist:\n",
    "\n",
    "        src = imread(os.path.join(datapath, fn),1)\n",
    "        img = resize(src,(32,32),order = 3)\n",
    "        \n",
    "        images[ii,:,:,0] = img\n",
    "        cc = -1\n",
    "        for cl in range(len(classes)):\n",
    "            if fn[-5] == classes[cl]:\n",
    "                cc = cl\n",
    "        Y[ii,cc]=1\n",
    "        ii += 1\n",
    "\n",
    "    BaseImages = images\n",
    "    BaseY = Y\n",
    "    return BaseImages, BaseY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_train_and_val(datapath):\n",
    "    # This part reads the images\n",
    "    classes = ['b','c','l','h']\n",
    "    imagelist = [fn for fn in os.listdir(datapath)]\n",
    "    N = len(imagelist)\n",
    "    num_classes = len(classes)\n",
    "    images = np.zeros((N, 32, 32, 1))\n",
    "    Y = np.zeros((N,num_classes))\n",
    "    ii=0\n",
    "    for fn in imagelist:\n",
    "\n",
    "        images[ii,:,:,0] = imread(os.path.join(datapath, fn),1)\n",
    "        cc = -1\n",
    "        for cl in range(len(classes)):\n",
    "            if fn[-5] == classes[cl]:\n",
    "                cc = cl\n",
    "        Y[ii,cc]=1\n",
    "        ii += 1\n",
    "\n",
    "    return images, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the data for training and validation:\n",
    "src_data = '/MLdata/MLcourse/X_ray/'\n",
    "train_path = src_data + 'train'\n",
    "val_path = src_data + 'validation'\n",
    "test_path = src_data + 'test'\n",
    "BaseX_train , BaseY_train = preprocess_train_and_val(train_path)\n",
    "BaseX_val , BaseY_val = preprocess_train_and_val(val_path)\n",
    "X_test, Y_test = preprocess(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 1: Fully connected layers \n",
    "--------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:red\">***Task 1:***</span> *NN with fully connected layers. \n",
    "\n",
    "Elaborate a NN with 2 hidden fully connected layers with 300, 150 neurons and 4 neurons for classification. Use ReLU activation functions for the hidden layers and He_normal for initialization. Don't forget to flatten your image before feedforward to the first dense layer. Name the model `model_relu`.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/smorandv/anaconda3/envs/2ndPaper-t2/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "#--------------------------Impelment your code here:-------------------------------------\n",
    "n_layers = 300\n",
    "n_classes = 4\n",
    "input_shape = (32,32,1)\n",
    "\n",
    "model_relu = Sequential()\n",
    "model_relu.add(Flatten(input_shape=input_shape))\n",
    "\n",
    "model_relu.add(Dense(n_layers, activation='relu', kernel_initializer='he_normal'))\n",
    "model_relu.add(Dense(n_layers/2, activation='relu', kernel_initializer='he_normal'))\n",
    "\n",
    "model_relu.add(Dense(n_classes))\n",
    "model_relu.add(Activation('softmax'))\n",
    "\n",
    "#----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300)               307500    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 150)               45150     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 604       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 353,254\n",
      "Trainable params: 353,254\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_relu.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputs: \n",
    "input_shape = (32,32,1)\n",
    "learn_rate = 1e-5\n",
    "decay = 0\n",
    "batch_size = 64\n",
    "epochs = 25\n",
    "\n",
    "#Define your optimizar parameters:\n",
    "AdamOpt = Adam(lr=learn_rate,decay=decay)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model with the optimizer above, accuracy metric and adequate loss for multiclass task. Train your model on the training set and evaluate the model on the testing set. Print the accuracy and loss over the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved initialized model at results/init_weigths_MLP_relu_task1.h5 \n",
      "Train on 6474 samples, validate on 1728 samples\n",
      "WARNING:tensorflow:From /home/smorandv/anaconda3/envs/2ndPaper-t2/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/25\n",
      "6474/6474 [==============================] - 1s 168us/sample - loss: 1.3429 - acc: 0.3803 - val_loss: 1.2153 - val_acc: 0.5156\n",
      "Epoch 2/25\n",
      "6474/6474 [==============================] - 1s 98us/sample - loss: 1.1582 - acc: 0.5726 - val_loss: 1.0917 - val_acc: 0.6308\n",
      "Epoch 3/25\n",
      "6474/6474 [==============================] - 1s 95us/sample - loss: 1.0563 - acc: 0.6535 - val_loss: 1.0075 - val_acc: 0.6846\n",
      "Epoch 4/25\n",
      "6474/6474 [==============================] - 1s 94us/sample - loss: 0.9837 - acc: 0.7053 - val_loss: 0.9474 - val_acc: 0.7292\n",
      "Epoch 5/25\n",
      "6474/6474 [==============================] - 1s 99us/sample - loss: 0.9268 - acc: 0.7340 - val_loss: 0.8994 - val_acc: 0.7436\n",
      "Epoch 6/25\n",
      "6474/6474 [==============================] - 1s 104us/sample - loss: 0.8833 - acc: 0.7495 - val_loss: 0.8608 - val_acc: 0.7529\n",
      "Epoch 7/25\n",
      "6474/6474 [==============================] - 1s 102us/sample - loss: 0.8452 - acc: 0.7573 - val_loss: 0.8281 - val_acc: 0.7650\n",
      "Epoch 8/25\n",
      "6474/6474 [==============================] - 1s 104us/sample - loss: 0.8137 - acc: 0.7672 - val_loss: 0.7996 - val_acc: 0.7691\n",
      "Epoch 9/25\n",
      "6474/6474 [==============================] - 1s 99us/sample - loss: 0.7856 - acc: 0.7705 - val_loss: 0.7770 - val_acc: 0.7731\n",
      "Epoch 10/25\n",
      "6474/6474 [==============================] - 1s 106us/sample - loss: 0.7605 - acc: 0.7757 - val_loss: 0.7543 - val_acc: 0.7772\n",
      "Epoch 11/25\n",
      "6474/6474 [==============================] - 1s 102us/sample - loss: 0.7374 - acc: 0.7816 - val_loss: 0.7369 - val_acc: 0.7749\n",
      "Epoch 12/25\n",
      "6474/6474 [==============================] - 1s 100us/sample - loss: 0.7167 - acc: 0.7898 - val_loss: 0.7133 - val_acc: 0.7876\n",
      "Epoch 13/25\n",
      "6474/6474 [==============================] - 1s 102us/sample - loss: 0.6964 - acc: 0.7930 - val_loss: 0.6955 - val_acc: 0.7940\n",
      "Epoch 14/25\n",
      "6474/6474 [==============================] - 1s 100us/sample - loss: 0.6778 - acc: 0.7980 - val_loss: 0.6784 - val_acc: 0.7911\n",
      "Epoch 15/25\n",
      "6474/6474 [==============================] - 1s 101us/sample - loss: 0.6612 - acc: 0.8040 - val_loss: 0.6641 - val_acc: 0.7975\n",
      "Epoch 16/25\n",
      "6474/6474 [==============================] - 1s 102us/sample - loss: 0.6459 - acc: 0.8102 - val_loss: 0.6500 - val_acc: 0.8009\n",
      "Epoch 17/25\n",
      "6474/6474 [==============================] - 1s 103us/sample - loss: 0.6297 - acc: 0.8111 - val_loss: 0.6351 - val_acc: 0.8050\n",
      "Epoch 18/25\n",
      "6474/6474 [==============================] - 1s 103us/sample - loss: 0.6159 - acc: 0.8185 - val_loss: 0.6246 - val_acc: 0.8027\n",
      "Epoch 19/25\n",
      "6474/6474 [==============================] - 1s 99us/sample - loss: 0.6034 - acc: 0.8199 - val_loss: 0.6097 - val_acc: 0.8137\n",
      "Epoch 20/25\n",
      "6474/6474 [==============================] - 1s 101us/sample - loss: 0.5908 - acc: 0.8234 - val_loss: 0.6003 - val_acc: 0.8137\n",
      "Epoch 21/25\n",
      "6474/6474 [==============================] - 1s 99us/sample - loss: 0.5777 - acc: 0.8281 - val_loss: 0.5860 - val_acc: 0.8171\n",
      "Epoch 22/25\n",
      "6474/6474 [==============================] - 1s 101us/sample - loss: 0.5657 - acc: 0.8316 - val_loss: 0.5762 - val_acc: 0.8223\n",
      "Epoch 23/25\n",
      "6474/6474 [==============================] - 1s 100us/sample - loss: 0.5551 - acc: 0.8353 - val_loss: 0.5669 - val_acc: 0.8223\n",
      "Epoch 24/25\n",
      "6474/6474 [==============================] - 1s 105us/sample - loss: 0.5448 - acc: 0.8378 - val_loss: 0.5573 - val_acc: 0.8264\n",
      "Epoch 25/25\n",
      "6474/6474 [==============================] - 1s 99us/sample - loss: 0.5343 - acc: 0.8400 - val_loss: 0.5502 - val_acc: 0.8287\n",
      " - 0s - loss: 0.7856 - acc: 0.6971\n",
      "Saved trained model at results/final_weights_MLP_relu_25_epochs.h5 \n",
      "\n",
      "The loss is: 0.79\n",
      "The accuracy is: 0.7\n"
     ]
    }
   ],
   "source": [
    "#--------------------------Impelment your code here:-------------------------------------\n",
    "model_relu.compile(optimizer=AdamOpt, metrics=['accuracy'], loss='categorical_crossentropy')\n",
    "\n",
    "# saving the initial weights\n",
    "if not(\"results\" in os.listdir()):\n",
    "    os.mkdir(\"results\")\n",
    "save_dir = \"results/\"\n",
    "model_name = \"init_weigths_MLP_relu_task1.h5\"\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model_relu.save(model_path)\n",
    "print('Saved initialized model at %s ' % model_path)\n",
    "\n",
    "history = model_relu.fit(BaseX_train, BaseY_train, batch_size=batch_size, epochs=epochs, validation_split=0, validation_data = (BaseX_val, BaseY_val), shuffle=True)\n",
    "loss_and_metrics = model_relu.evaluate(X_test, Y_test, verbose=2)\n",
    "\n",
    "# saving the final weights\n",
    "if not(\"results\" in os.listdir()):\n",
    "    os.mkdir(\"results\")\n",
    "save_dir = \"results/\"\n",
    "model_name = \"final_weights_MLP_relu_25_epochs.h5\"\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model_relu.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "print()\n",
    "print('The loss is:', np.round(loss_and_metrics[0], 2))\n",
    "print('The accuracy is:', np.round(loss_and_metrics[1], 2))\n",
    "#----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:red\">***Task 2:***</span> *Activation functions.* \n",
    "\n",
    "**Change the activation functions to LeakyRelu or tanh or sigmoid. Name the new model `new_a_model`. Explain how it can affect the model.**\n",
    "\n",
    "We changed the activation functions to tanh and it can effect the model in some ways:\n",
    "* In tanh we can see the vanishing gradient problem - the gradient can be very small and prevent a weight from changing its value, while ReLU avoids and rectifies this problem.\n",
    "* The proccess can be slower. ReLu is less computationally expensive than tanh because it involves simpler mathematical operations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------Impelment your code here:-------------------------------------\n",
    "n_layers = 300\n",
    "n_classes = 4\n",
    "input_shape = (32,32,1)\n",
    "\n",
    "new_a_model = Sequential()\n",
    "new_a_model.add(Flatten(input_shape=input_shape))\n",
    "\n",
    "new_a_model.add(Dense(n_layers, activation='tanh', kernel_initializer='he_normal'))\n",
    "new_a_model.add(Dense(n_layers/2, activation='tanh', kernel_initializer='he_normal'))\n",
    "\n",
    "new_a_model.add(Dense(n_classes))\n",
    "new_a_model.add(Activation('softmax'))\n",
    "#----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 300)               307500    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 150)               45150     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4)                 604       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 353,254\n",
      "Trainable params: 353,254\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_a_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:red\">***Task 3:***</span> *Number of epochs.* \n",
    "\n",
    "**Train the new model using 25 and 40 epochs. What difference does it makes in term of performance? Remember to save the compiled model for having initialized weights for every run as we did in tutorial 12. Evaluate each trained model on the test set**\n",
    "\n",
    "In term of performance, greater number of epochs will get better results in the training set (high accuracy), but we will risk over-fitting which result in low accuracy on the training set. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputs: \n",
    "input_shape = (32,32,1)\n",
    "learn_rate = 1e-5\n",
    "decay = 0\n",
    "batch_size = 64\n",
    "epochs = 25\n",
    "\n",
    "#Defining the optimizar parameters:\n",
    "AdamOpt = Adam(lr=learn_rate,decay=decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved initialized model at results/init_weigths_MLP_tanh_task3.h5 \n",
      "Train on 6474 samples, validate on 1728 samples\n",
      "Epoch 1/25\n",
      "6474/6474 [==============================] - 1s 144us/sample - loss: 1.2664 - acc: 0.4425 - val_loss: 1.1626 - val_acc: 0.5689\n",
      "Epoch 2/25\n",
      "6474/6474 [==============================] - 1s 102us/sample - loss: 1.0991 - acc: 0.6095 - val_loss: 1.0506 - val_acc: 0.6221\n",
      "Epoch 3/25\n",
      "6474/6474 [==============================] - 1s 103us/sample - loss: 1.0106 - acc: 0.6498 - val_loss: 0.9830 - val_acc: 0.6649\n",
      "Epoch 4/25\n",
      "6474/6474 [==============================] - 1s 102us/sample - loss: 0.9497 - acc: 0.6736 - val_loss: 0.9332 - val_acc: 0.6916\n",
      "Epoch 5/25\n",
      "6474/6474 [==============================] - 1s 103us/sample - loss: 0.9064 - acc: 0.6945 - val_loss: 0.8967 - val_acc: 0.6979\n",
      "Epoch 6/25\n",
      "6474/6474 [==============================] - 1s 104us/sample - loss: 0.8694 - acc: 0.7045 - val_loss: 0.8655 - val_acc: 0.7141\n",
      "Epoch 7/25\n",
      "6474/6474 [==============================] - 1s 107us/sample - loss: 0.8403 - acc: 0.7129 - val_loss: 0.8430 - val_acc: 0.7153\n",
      "Epoch 8/25\n",
      "6474/6474 [==============================] - 1s 102us/sample - loss: 0.8166 - acc: 0.7178 - val_loss: 0.8213 - val_acc: 0.7269\n",
      "Epoch 9/25\n",
      "6474/6474 [==============================] - 1s 105us/sample - loss: 0.7945 - acc: 0.7269 - val_loss: 0.8024 - val_acc: 0.7315\n",
      "Epoch 10/25\n",
      "6474/6474 [==============================] - 1s 103us/sample - loss: 0.7758 - acc: 0.7308 - val_loss: 0.7848 - val_acc: 0.7367\n",
      "Epoch 11/25\n",
      "6474/6474 [==============================] - 1s 97us/sample - loss: 0.7578 - acc: 0.7385 - val_loss: 0.7702 - val_acc: 0.7454\n",
      "Epoch 12/25\n",
      "6474/6474 [==============================] - 1s 102us/sample - loss: 0.7426 - acc: 0.7434 - val_loss: 0.7577 - val_acc: 0.7431\n",
      "Epoch 13/25\n",
      "6474/6474 [==============================] - 1s 107us/sample - loss: 0.7276 - acc: 0.7492 - val_loss: 0.7464 - val_acc: 0.7436\n",
      "Epoch 14/25\n",
      "6474/6474 [==============================] - 1s 102us/sample - loss: 0.7141 - acc: 0.7539 - val_loss: 0.7309 - val_acc: 0.7535\n",
      "Epoch 15/25\n",
      "6474/6474 [==============================] - 1s 106us/sample - loss: 0.7020 - acc: 0.7566 - val_loss: 0.7207 - val_acc: 0.7593\n",
      "Epoch 16/25\n",
      "6474/6474 [==============================] - 1s 103us/sample - loss: 0.6898 - acc: 0.7648 - val_loss: 0.7105 - val_acc: 0.7598\n",
      "Epoch 17/25\n",
      "6474/6474 [==============================] - 1s 102us/sample - loss: 0.6793 - acc: 0.7677 - val_loss: 0.7018 - val_acc: 0.7633\n",
      "Epoch 18/25\n",
      "6474/6474 [==============================] - 1s 100us/sample - loss: 0.6686 - acc: 0.7720 - val_loss: 0.6908 - val_acc: 0.7622\n",
      "Epoch 19/25\n",
      "6474/6474 [==============================] - 1s 99us/sample - loss: 0.6598 - acc: 0.7715 - val_loss: 0.6826 - val_acc: 0.7714\n",
      "Epoch 20/25\n",
      "6474/6474 [==============================] - 1s 101us/sample - loss: 0.6499 - acc: 0.7800 - val_loss: 0.6744 - val_acc: 0.7708\n",
      "Epoch 21/25\n",
      "6474/6474 [==============================] - 1s 105us/sample - loss: 0.6406 - acc: 0.7842 - val_loss: 0.6702 - val_acc: 0.7749\n",
      "Epoch 22/25\n",
      "6474/6474 [==============================] - 1s 110us/sample - loss: 0.6327 - acc: 0.7859 - val_loss: 0.6596 - val_acc: 0.7818\n",
      "Epoch 23/25\n",
      "6474/6474 [==============================] - 1s 99us/sample - loss: 0.6242 - acc: 0.7851 - val_loss: 0.6510 - val_acc: 0.7830\n",
      "Epoch 24/25\n",
      "6474/6474 [==============================] - 1s 99us/sample - loss: 0.6165 - acc: 0.7893 - val_loss: 0.6433 - val_acc: 0.7795\n",
      "Epoch 25/25\n",
      "6474/6474 [==============================] - 1s 102us/sample - loss: 0.6087 - acc: 0.7947 - val_loss: 0.6379 - val_acc: 0.7824\n",
      " - 0s - loss: 0.8204 - acc: 0.6457\n",
      "Saved trained model at results/final_weights_MLP_tanh_25_epochs.h5 \n",
      "\n",
      "The loss is: 0.82\n",
      "The accuracy is: 0.65\n"
     ]
    }
   ],
   "source": [
    "#--------------------------Impelment your code here:-------------------------------------\n",
    "new_a_model.compile(optimizer=AdamOpt, metrics=['accuracy'], loss='categorical_crossentropy')\n",
    "\n",
    "# saving the initial weights\n",
    "if not(\"results\" in os.listdir()):\n",
    "    os.mkdir(\"results\")\n",
    "save_dir = \"results/\"\n",
    "model_name = \"init_weigths_MLP_tanh_task3.h5\"\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "new_a_model.save(model_path)\n",
    "print('Saved initialized model at %s ' % model_path)\n",
    "\n",
    "history = new_a_model.fit(BaseX_train, BaseY_train, batch_size=batch_size, epochs=epochs, validation_split=0, validation_data = (BaseX_val, BaseY_val), shuffle=True)\n",
    "loss_and_metrics = new_a_model.evaluate(X_test, Y_test, verbose=2)\n",
    "\n",
    "# saving the final weights\n",
    "if not(\"results\" in os.listdir()):\n",
    "    os.mkdir(\"results\")\n",
    "save_dir = \"results/\"\n",
    "model_name = \"final_weights_MLP_tanh_25_epochs.h5\"\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "new_a_model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "print()\n",
    "print('The loss is:', np.round(loss_and_metrics[0], 2))\n",
    "print('The accuracy is:', np.round(loss_and_metrics[1], 2))\n",
    "#-----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputs: \n",
    "input_shape = (32,32,1)\n",
    "learn_rate = 1e-5\n",
    "decay = 0\n",
    "batch_size = 64\n",
    "epochs = 40\n",
    "# epochs = 50  # check for task 4: Batch normalization question.\n",
    "\n",
    "#Defining the optimizar parameters:\n",
    "AdamOpt = Adam(lr=learn_rate,decay=decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6474 samples, validate on 1728 samples\n",
      "Epoch 1/40\n",
      "6474/6474 [==============================] - 1s 140us/sample - loss: 1.2677 - acc: 0.4370 - val_loss: 1.1602 - val_acc: 0.5718\n",
      "Epoch 2/40\n",
      "6474/6474 [==============================] - 1s 106us/sample - loss: 1.1000 - acc: 0.6009 - val_loss: 1.0516 - val_acc: 0.6273\n",
      "Epoch 3/40\n",
      "6474/6474 [==============================] - 1s 101us/sample - loss: 1.0137 - acc: 0.6452 - val_loss: 0.9859 - val_acc: 0.6701\n",
      "Epoch 4/40\n",
      "6474/6474 [==============================] - 1s 101us/sample - loss: 0.9554 - acc: 0.6739 - val_loss: 0.9369 - val_acc: 0.6806\n",
      "Epoch 5/40\n",
      "6474/6474 [==============================] - 1s 101us/sample - loss: 0.9097 - acc: 0.6901 - val_loss: 0.9053 - val_acc: 0.6979\n",
      "Epoch 6/40\n",
      "6474/6474 [==============================] - 1s 103us/sample - loss: 0.8749 - acc: 0.7025 - val_loss: 0.8722 - val_acc: 0.7124\n",
      "Epoch 7/40\n",
      "6474/6474 [==============================] - 1s 104us/sample - loss: 0.8462 - acc: 0.7115 - val_loss: 0.8460 - val_acc: 0.7153\n",
      "Epoch 8/40\n",
      "6474/6474 [==============================] - 1s 105us/sample - loss: 0.8203 - acc: 0.7195 - val_loss: 0.8258 - val_acc: 0.7303\n",
      "Epoch 9/40\n",
      "6474/6474 [==============================] - 1s 103us/sample - loss: 0.7994 - acc: 0.7241 - val_loss: 0.8065 - val_acc: 0.7297\n",
      "Epoch 10/40\n",
      "6474/6474 [==============================] - 1s 100us/sample - loss: 0.7810 - acc: 0.7315 - val_loss: 0.7900 - val_acc: 0.7373\n",
      "Epoch 11/40\n",
      "6474/6474 [==============================] - 1s 102us/sample - loss: 0.7626 - acc: 0.7348 - val_loss: 0.7744 - val_acc: 0.7373\n",
      "Epoch 12/40\n",
      "6474/6474 [==============================] - 1s 104us/sample - loss: 0.7461 - acc: 0.7441 - val_loss: 0.7626 - val_acc: 0.7436\n",
      "Epoch 13/40\n",
      "6474/6474 [==============================] - 1s 100us/sample - loss: 0.7327 - acc: 0.7481 - val_loss: 0.7490 - val_acc: 0.7436\n",
      "Epoch 14/40\n",
      "6474/6474 [==============================] - 1s 103us/sample - loss: 0.7190 - acc: 0.7518 - val_loss: 0.7354 - val_acc: 0.7517\n",
      "Epoch 15/40\n",
      "6474/6474 [==============================] - 1s 102us/sample - loss: 0.7061 - acc: 0.7559 - val_loss: 0.7247 - val_acc: 0.7575\n",
      "Epoch 16/40\n",
      "6474/6474 [==============================] - 1s 108us/sample - loss: 0.6942 - acc: 0.7621 - val_loss: 0.7143 - val_acc: 0.7593\n",
      "Epoch 17/40\n",
      "6474/6474 [==============================] - 1s 103us/sample - loss: 0.6835 - acc: 0.7640 - val_loss: 0.7049 - val_acc: 0.7645\n",
      "Epoch 18/40\n",
      "6474/6474 [==============================] - 1s 99us/sample - loss: 0.6728 - acc: 0.7692 - val_loss: 0.6953 - val_acc: 0.7650\n",
      "Epoch 19/40\n",
      "6474/6474 [==============================] - 1s 100us/sample - loss: 0.6640 - acc: 0.7722 - val_loss: 0.6889 - val_acc: 0.7731\n",
      "Epoch 20/40\n",
      "6474/6474 [==============================] - 1s 102us/sample - loss: 0.6534 - acc: 0.7743 - val_loss: 0.6820 - val_acc: 0.7749\n",
      "Epoch 21/40\n",
      "6474/6474 [==============================] - 1s 106us/sample - loss: 0.6444 - acc: 0.7807 - val_loss: 0.6690 - val_acc: 0.7726\n",
      "Epoch 22/40\n",
      "6474/6474 [==============================] - 1s 104us/sample - loss: 0.6362 - acc: 0.7821 - val_loss: 0.6667 - val_acc: 0.7766\n",
      "Epoch 23/40\n",
      "6474/6474 [==============================] - 1s 106us/sample - loss: 0.6275 - acc: 0.7875 - val_loss: 0.6547 - val_acc: 0.7830\n",
      "Epoch 24/40\n",
      "6474/6474 [==============================] - 1s 101us/sample - loss: 0.6202 - acc: 0.7879 - val_loss: 0.6488 - val_acc: 0.7882\n",
      "Epoch 25/40\n",
      "6474/6474 [==============================] - 1s 100us/sample - loss: 0.6118 - acc: 0.7929 - val_loss: 0.6448 - val_acc: 0.7789\n",
      "Epoch 26/40\n",
      "6474/6474 [==============================] - 1s 102us/sample - loss: 0.6045 - acc: 0.7960 - val_loss: 0.6331 - val_acc: 0.7853\n",
      "Epoch 27/40\n",
      "6474/6474 [==============================] - 1s 100us/sample - loss: 0.5978 - acc: 0.7960 - val_loss: 0.6269 - val_acc: 0.7940\n",
      "Epoch 28/40\n",
      "6474/6474 [==============================] - 1s 102us/sample - loss: 0.5907 - acc: 0.8006 - val_loss: 0.6222 - val_acc: 0.7946\n",
      "Epoch 29/40\n",
      "6474/6474 [==============================] - 1s 104us/sample - loss: 0.5840 - acc: 0.8020 - val_loss: 0.6157 - val_acc: 0.7963\n",
      "Epoch 30/40\n",
      "6474/6474 [==============================] - 1s 104us/sample - loss: 0.5778 - acc: 0.8024 - val_loss: 0.6092 - val_acc: 0.7963\n",
      "Epoch 31/40\n",
      "6474/6474 [==============================] - 1s 102us/sample - loss: 0.5705 - acc: 0.8089 - val_loss: 0.6011 - val_acc: 0.8003\n",
      "Epoch 32/40\n",
      "6474/6474 [==============================] - 1s 102us/sample - loss: 0.5652 - acc: 0.8061 - val_loss: 0.6016 - val_acc: 0.8021\n",
      "Epoch 33/40\n",
      "6474/6474 [==============================] - 1s 106us/sample - loss: 0.5590 - acc: 0.8122 - val_loss: 0.5960 - val_acc: 0.8003\n",
      "Epoch 34/40\n",
      "6474/6474 [==============================] - 1s 102us/sample - loss: 0.5537 - acc: 0.8122 - val_loss: 0.5883 - val_acc: 0.8015\n",
      "Epoch 35/40\n",
      "6474/6474 [==============================] - 1s 103us/sample - loss: 0.5470 - acc: 0.8153 - val_loss: 0.5802 - val_acc: 0.8084\n",
      "Epoch 36/40\n",
      "6474/6474 [==============================] - 1s 96us/sample - loss: 0.5421 - acc: 0.8180 - val_loss: 0.5745 - val_acc: 0.8125\n",
      "Epoch 37/40\n",
      "6474/6474 [==============================] - 1s 96us/sample - loss: 0.5363 - acc: 0.8222 - val_loss: 0.5725 - val_acc: 0.8084\n",
      "Epoch 38/40\n",
      "6474/6474 [==============================] - 1s 97us/sample - loss: 0.5312 - acc: 0.8214 - val_loss: 0.5706 - val_acc: 0.8061\n",
      "Epoch 39/40\n",
      "6474/6474 [==============================] - 1s 104us/sample - loss: 0.5255 - acc: 0.8265 - val_loss: 0.5597 - val_acc: 0.8171\n",
      "Epoch 40/40\n",
      "6474/6474 [==============================] - 1s 104us/sample - loss: 0.5204 - acc: 0.8281 - val_loss: 0.5561 - val_acc: 0.8171\n",
      " - 0s - loss: 0.8174 - acc: 0.6571\n",
      "Saved trained model at results/final_weights_MLP_tanh_40_epochs.h5 \n",
      "\n",
      "The loss is: 0.82\n",
      "The accuracy is: 0.66\n"
     ]
    }
   ],
   "source": [
    "#--------------------------Impelment your code here:-------------------------------------\n",
    "from tensorflow.keras.models import load_model\n",
    "new_a_model = load_model(\"results/init_weigths_MLP_tanh_task3.h5\")\n",
    "\n",
    "history = new_a_model.fit(BaseX_train, BaseY_train, batch_size=batch_size, epochs=epochs, validation_split=0, validation_data = (BaseX_val, BaseY_val), shuffle=True)\n",
    "loss_and_metrics = new_a_model.evaluate(X_test, Y_test, verbose=2)\n",
    "\n",
    "# saving the final weights\n",
    "if not(\"results\" in os.listdir()):\n",
    "    os.mkdir(\"results\")\n",
    "save_dir = \"results/\"\n",
    "model_name = \"final_weights_MLP_tanh_40_epochs.h5\"\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "new_a_model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "print()\n",
    "print('The loss is:', np.round(loss_and_metrics[0], 2))\n",
    "print('The accuracy is:', np.round(loss_and_metrics[1], 2))\n",
    "#-----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:red\">***Task 4:***</span> *Mini-batches.* \n",
    "\n",
    "**Build the `model_relu` again and run it with a batch size of 32 instead of 64. What are the advantages of the mini-batch vs. SGD?**\n",
    "\n",
    "The advantages of mini-batch over SGD are:\n",
    "* Higher accuracy\n",
    "* Better computational efficiency \n",
    "* Mini-batch perform weight updates more often than SGD, so we achieve faster learning.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/smorandv/anaconda3/envs/2ndPaper-t2/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "#--------------------------Impelment your code here:-------------------------------------\n",
    "n_layers = 300\n",
    "n_classes = 4\n",
    "input_shape = (32,32,1)\n",
    "\n",
    "model_relu = Sequential()\n",
    "model_relu.add(Flatten(input_shape=input_shape))\n",
    "\n",
    "model_relu.add(Dense(n_layers, activation='relu', kernel_initializer='he_normal'))\n",
    "model_relu.add(Dropout(0.2))\n",
    "\n",
    "model_relu.add(Dense(n_layers/2, activation='relu', kernel_initializer='he_normal'))\n",
    "model_relu.add(Dropout(0.2))\n",
    "\n",
    "model_relu.add(Dense(n_classes))\n",
    "model_relu.add(Activation('softmax'))\n",
    "#----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 50\n",
    "learn_rate = 1e-5\n",
    "decay = 0\n",
    "\n",
    "#Define your optimizar parameters:\n",
    "AdamOpt = Adam(lr=learn_rate,decay=decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved initialized model at results/init_weigths_MLP_relu_task4.h5 \n",
      "Train on 6474 samples, validate on 1728 samples\n",
      "Epoch 1/50\n",
      "6474/6474 [==============================] - 2s 256us/sample - loss: 1.3362 - acc: 0.3761 - val_loss: 1.1220 - val_acc: 0.6019\n",
      "Epoch 2/50\n",
      "6474/6474 [==============================] - 1s 218us/sample - loss: 1.1394 - acc: 0.5187 - val_loss: 1.0236 - val_acc: 0.6539\n",
      "Epoch 3/50\n",
      "6474/6474 [==============================] - 1s 214us/sample - loss: 1.0561 - acc: 0.5783 - val_loss: 0.9671 - val_acc: 0.6771\n",
      "Epoch 4/50\n",
      "6474/6474 [==============================] - 1s 209us/sample - loss: 0.9951 - acc: 0.6171 - val_loss: 0.9131 - val_acc: 0.7072\n",
      "Epoch 5/50\n",
      "6474/6474 [==============================] - 1s 226us/sample - loss: 0.9504 - acc: 0.6470 - val_loss: 0.8705 - val_acc: 0.7170\n",
      "Epoch 6/50\n",
      "6474/6474 [==============================] - 1s 216us/sample - loss: 0.9088 - acc: 0.6735 - val_loss: 0.8388 - val_acc: 0.7263\n",
      "Epoch 7/50\n",
      "6474/6474 [==============================] - 1s 223us/sample - loss: 0.8758 - acc: 0.6850 - val_loss: 0.8100 - val_acc: 0.7459\n",
      "Epoch 8/50\n",
      "6474/6474 [==============================] - 1s 224us/sample - loss: 0.8450 - acc: 0.7005 - val_loss: 0.7851 - val_acc: 0.7512\n",
      "Epoch 9/50\n",
      "6474/6474 [==============================] - 1s 217us/sample - loss: 0.8234 - acc: 0.7150 - val_loss: 0.7591 - val_acc: 0.7650\n",
      "Epoch 10/50\n",
      "6474/6474 [==============================] - 1s 219us/sample - loss: 0.7906 - acc: 0.7308 - val_loss: 0.7379 - val_acc: 0.7668\n",
      "Epoch 11/50\n",
      "6474/6474 [==============================] - 1s 218us/sample - loss: 0.7723 - acc: 0.7357 - val_loss: 0.7171 - val_acc: 0.7737\n",
      "Epoch 12/50\n",
      "6474/6474 [==============================] - 1s 208us/sample - loss: 0.7526 - acc: 0.7527 - val_loss: 0.6969 - val_acc: 0.7755\n",
      "Epoch 13/50\n",
      "6474/6474 [==============================] - 1s 213us/sample - loss: 0.7293 - acc: 0.7578 - val_loss: 0.6768 - val_acc: 0.7830\n",
      "Epoch 14/50\n",
      "6474/6474 [==============================] - 1s 216us/sample - loss: 0.7123 - acc: 0.7649 - val_loss: 0.6618 - val_acc: 0.7888\n",
      "Epoch 15/50\n",
      "6474/6474 [==============================] - 1s 211us/sample - loss: 0.6970 - acc: 0.7703 - val_loss: 0.6454 - val_acc: 0.7917\n",
      "Epoch 16/50\n",
      "6474/6474 [==============================] - 1s 216us/sample - loss: 0.6777 - acc: 0.7748 - val_loss: 0.6273 - val_acc: 0.8009\n",
      "Epoch 17/50\n",
      "6474/6474 [==============================] - 1s 215us/sample - loss: 0.6633 - acc: 0.7845 - val_loss: 0.6130 - val_acc: 0.8038\n",
      "Epoch 18/50\n",
      "6474/6474 [==============================] - 1s 207us/sample - loss: 0.6481 - acc: 0.7848 - val_loss: 0.5983 - val_acc: 0.8084\n",
      "Epoch 19/50\n",
      "6474/6474 [==============================] - 1s 210us/sample - loss: 0.6277 - acc: 0.7949 - val_loss: 0.5868 - val_acc: 0.8137\n",
      "Epoch 20/50\n",
      "6474/6474 [==============================] - 1s 214us/sample - loss: 0.6226 - acc: 0.7970 - val_loss: 0.5743 - val_acc: 0.8148\n",
      "Epoch 21/50\n",
      "6474/6474 [==============================] - 1s 214us/sample - loss: 0.6070 - acc: 0.8006 - val_loss: 0.5657 - val_acc: 0.8171\n",
      "Epoch 22/50\n",
      "6474/6474 [==============================] - 1s 216us/sample - loss: 0.5961 - acc: 0.8065 - val_loss: 0.5523 - val_acc: 0.8235\n",
      "Epoch 23/50\n",
      "6474/6474 [==============================] - 1s 226us/sample - loss: 0.5832 - acc: 0.8080 - val_loss: 0.5412 - val_acc: 0.8275\n",
      "Epoch 24/50\n",
      "6474/6474 [==============================] - 1s 217us/sample - loss: 0.5737 - acc: 0.8160 - val_loss: 0.5365 - val_acc: 0.8241\n",
      "Epoch 25/50\n",
      "6474/6474 [==============================] - 1s 218us/sample - loss: 0.5603 - acc: 0.8176 - val_loss: 0.5274 - val_acc: 0.8310\n",
      "Epoch 26/50\n",
      "6474/6474 [==============================] - 1s 223us/sample - loss: 0.5513 - acc: 0.8210 - val_loss: 0.5155 - val_acc: 0.8322\n",
      "Epoch 27/50\n",
      "6474/6474 [==============================] - 1s 208us/sample - loss: 0.5410 - acc: 0.8251 - val_loss: 0.5110 - val_acc: 0.8328\n",
      "Epoch 28/50\n",
      "6474/6474 [==============================] - 1s 212us/sample - loss: 0.5345 - acc: 0.8278 - val_loss: 0.4992 - val_acc: 0.8403\n",
      "Epoch 29/50\n",
      "6474/6474 [==============================] - 1s 208us/sample - loss: 0.5172 - acc: 0.8346 - val_loss: 0.4907 - val_acc: 0.8438\n",
      "Epoch 30/50\n",
      "6474/6474 [==============================] - 1s 210us/sample - loss: 0.5114 - acc: 0.8343 - val_loss: 0.4838 - val_acc: 0.8426\n",
      "Epoch 31/50\n",
      "6474/6474 [==============================] - 1s 217us/sample - loss: 0.5029 - acc: 0.8352 - val_loss: 0.4758 - val_acc: 0.8472\n",
      "Epoch 32/50\n",
      "6474/6474 [==============================] - 1s 214us/sample - loss: 0.4930 - acc: 0.8369 - val_loss: 0.4700 - val_acc: 0.8490\n",
      "Epoch 33/50\n",
      "6474/6474 [==============================] - 1s 210us/sample - loss: 0.4949 - acc: 0.8381 - val_loss: 0.4633 - val_acc: 0.8501\n",
      "Epoch 34/50\n",
      "6474/6474 [==============================] - 1s 217us/sample - loss: 0.4811 - acc: 0.8440 - val_loss: 0.4567 - val_acc: 0.8519\n",
      "Epoch 35/50\n",
      "6474/6474 [==============================] - 1s 223us/sample - loss: 0.4718 - acc: 0.8466 - val_loss: 0.4504 - val_acc: 0.8513\n",
      "Epoch 36/50\n",
      "6474/6474 [==============================] - 1s 218us/sample - loss: 0.4713 - acc: 0.8452 - val_loss: 0.4481 - val_acc: 0.8524\n",
      "Epoch 37/50\n",
      "6474/6474 [==============================] - 1s 214us/sample - loss: 0.4610 - acc: 0.8455 - val_loss: 0.4400 - val_acc: 0.8576\n",
      "Epoch 38/50\n",
      "6474/6474 [==============================] - 1s 220us/sample - loss: 0.4510 - acc: 0.8533 - val_loss: 0.4340 - val_acc: 0.8594\n",
      "Epoch 39/50\n",
      "6474/6474 [==============================] - 1s 217us/sample - loss: 0.4524 - acc: 0.8542 - val_loss: 0.4299 - val_acc: 0.8600\n",
      "Epoch 40/50\n",
      "6474/6474 [==============================] - 1s 213us/sample - loss: 0.4483 - acc: 0.8520 - val_loss: 0.4278 - val_acc: 0.8588\n",
      "Epoch 41/50\n",
      "6474/6474 [==============================] - 1s 210us/sample - loss: 0.4351 - acc: 0.8627 - val_loss: 0.4222 - val_acc: 0.8623\n",
      "Epoch 42/50\n",
      "6474/6474 [==============================] - 1s 214us/sample - loss: 0.4332 - acc: 0.8577 - val_loss: 0.4201 - val_acc: 0.8646\n",
      "Epoch 43/50\n",
      "6474/6474 [==============================] - 1s 211us/sample - loss: 0.4270 - acc: 0.8571 - val_loss: 0.4163 - val_acc: 0.8652\n",
      "Epoch 44/50\n",
      "6474/6474 [==============================] - 1s 215us/sample - loss: 0.4229 - acc: 0.8594 - val_loss: 0.4103 - val_acc: 0.8657\n",
      "Epoch 45/50\n",
      "6474/6474 [==============================] - 1s 224us/sample - loss: 0.4179 - acc: 0.8604 - val_loss: 0.4058 - val_acc: 0.8698\n",
      "Epoch 46/50\n",
      "6474/6474 [==============================] - 1s 218us/sample - loss: 0.4166 - acc: 0.8610 - val_loss: 0.3999 - val_acc: 0.8733\n",
      "Epoch 47/50\n",
      "6474/6474 [==============================] - 1s 213us/sample - loss: 0.4152 - acc: 0.8652 - val_loss: 0.3977 - val_acc: 0.8709\n",
      "Epoch 48/50\n",
      "6474/6474 [==============================] - 1s 212us/sample - loss: 0.4085 - acc: 0.8655 - val_loss: 0.3954 - val_acc: 0.8733\n",
      "Epoch 49/50\n",
      "6474/6474 [==============================] - 1s 221us/sample - loss: 0.3994 - acc: 0.8659 - val_loss: 0.3921 - val_acc: 0.8738\n",
      "Epoch 50/50\n",
      "6474/6474 [==============================] - 1s 220us/sample - loss: 0.3921 - acc: 0.8719 - val_loss: 0.3868 - val_acc: 0.8756\n",
      " - 0s - loss: 0.8480 - acc: 0.6571\n",
      "Saved trained model at results/final_weights_MLP_relu_50_epochs.h5 \n",
      "\n",
      "The loss is: 0.85\n",
      "The accuracy is: 0.66\n"
     ]
    }
   ],
   "source": [
    "#--------------------------Impelment your code here:-------------------------------------\n",
    "model_relu.compile(optimizer=AdamOpt, metrics=['accuracy'], loss='categorical_crossentropy')\n",
    "\n",
    "# saving the initial weights\n",
    "if not(\"results\" in os.listdir()):\n",
    "    os.mkdir(\"results\")\n",
    "save_dir = \"results/\"\n",
    "model_name = \"init_weigths_MLP_relu_task4.h5\"\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model_relu.save(model_path)\n",
    "print('Saved initialized model at %s ' % model_path)\n",
    "\n",
    "history = model_relu.fit(BaseX_train, BaseY_train, batch_size=batch_size, epochs=epochs, validation_split=0, validation_data = (BaseX_val, BaseY_val), shuffle=True)\n",
    "loss_and_metrics = model_relu.evaluate(X_test, Y_test, verbose=2)\n",
    "\n",
    "# saving the final weights\n",
    "if not(\"results\" in os.listdir()):\n",
    "    os.mkdir(\"results\")\n",
    "save_dir = \"results/\"\n",
    "model_name = \"final_weights_MLP_relu_50_epochs.h5\"\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model_relu.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "print()\n",
    "print('The loss is:', np.round(loss_and_metrics[0], 2))\n",
    "print('The accuracy is:', np.round(loss_and_metrics[1], 2))\n",
    "#----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:red\">***Task 4:***</span> *Batch normalization.* \n",
    "\n",
    "**Build the `new_a_model` again and add batch normalization layers. How does it impact your results?**\n",
    "\n",
    "Batch normalization is a technique that standardizes the inputs to a layer for each mini-batch. This way, the model achieves convergence faster and stabilizing the learning process.\n",
    "\n",
    "\n",
    "We can see the impact on our results in the accuracy metric (for 50 epochs): \n",
    "\n",
    "With batch normalization: 0.6971\n",
    "\n",
    "Without batch normalization: 0.6629\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------Impelment your code here:-------------------------------------\n",
    "n_layers = 300\n",
    "n_classes = 4\n",
    "input_shape = (32,32,1)\n",
    "\n",
    "new_a_model = Sequential()\n",
    "new_a_model.add(Flatten(input_shape=input_shape))\n",
    "\n",
    "new_a_model.add(Dense(n_layers, activation='tanh', kernel_initializer='he_normal'))\n",
    "new_a_model.add(BatchNormalization())\n",
    "new_a_model.add(Dropout(0.2))\n",
    "\n",
    "new_a_model.add(Dense(n_layers/2, activation='tanh', kernel_initializer='he_normal'))\n",
    "new_a_model.add(BatchNormalization())\n",
    "new_a_model.add(Dropout(0.2))\n",
    "\n",
    "new_a_model.add(Dense(n_classes))\n",
    "new_a_model.add(Activation('softmax'))\n",
    "#---------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 50\n",
    "learn_rate = 1e-5\n",
    "decay = 0\n",
    "\n",
    "#Define your optimizar parameters:\n",
    "AdamOpt = Adam(lr=learn_rate,decay=decay)\n",
    "#Compile the network: \n",
    "new_a_model.compile(optimizer=AdamOpt, metrics=['accuracy'], loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved initialized model at results/init_weigths_MLP_tanh_task4.h5 \n",
      "Train on 6474 samples, validate on 1728 samples\n",
      "Epoch 1/50\n",
      "6474/6474 [==============================] - 2s 371us/sample - loss: 1.4114 - acc: 0.4568 - val_loss: 1.0204 - val_acc: 0.5961\n",
      "Epoch 2/50\n",
      "6474/6474 [==============================] - 2s 311us/sample - loss: 1.0474 - acc: 0.5947 - val_loss: 0.7925 - val_acc: 0.7188\n",
      "Epoch 3/50\n",
      "6474/6474 [==============================] - 2s 306us/sample - loss: 0.9327 - acc: 0.6413 - val_loss: 0.7081 - val_acc: 0.7471\n",
      "Epoch 4/50\n",
      "6474/6474 [==============================] - 2s 309us/sample - loss: 0.8498 - acc: 0.6773 - val_loss: 0.6399 - val_acc: 0.7778\n",
      "Epoch 5/50\n",
      "6474/6474 [==============================] - 2s 299us/sample - loss: 0.7875 - acc: 0.6959 - val_loss: 0.6129 - val_acc: 0.7940\n",
      "Epoch 6/50\n",
      "6474/6474 [==============================] - 2s 302us/sample - loss: 0.7469 - acc: 0.7206 - val_loss: 0.5677 - val_acc: 0.8056\n",
      "Epoch 7/50\n",
      "6474/6474 [==============================] - 2s 306us/sample - loss: 0.7210 - acc: 0.7311 - val_loss: 0.5755 - val_acc: 0.8079\n",
      "Epoch 8/50\n",
      "6474/6474 [==============================] - 2s 308us/sample - loss: 0.7086 - acc: 0.7332 - val_loss: 0.5082 - val_acc: 0.8252\n",
      "Epoch 9/50\n",
      "6474/6474 [==============================] - 2s 311us/sample - loss: 0.6614 - acc: 0.7490 - val_loss: 0.4975 - val_acc: 0.8310\n",
      "Epoch 10/50\n",
      "6474/6474 [==============================] - 2s 307us/sample - loss: 0.6377 - acc: 0.7651 - val_loss: 0.4763 - val_acc: 0.8414\n",
      "Epoch 11/50\n",
      "6474/6474 [==============================] - 2s 308us/sample - loss: 0.6240 - acc: 0.7714 - val_loss: 0.4733 - val_acc: 0.8495\n",
      "Epoch 12/50\n",
      "6474/6474 [==============================] - 2s 310us/sample - loss: 0.6117 - acc: 0.7715 - val_loss: 0.4484 - val_acc: 0.8519\n",
      "Epoch 13/50\n",
      "6474/6474 [==============================] - 2s 303us/sample - loss: 0.5986 - acc: 0.7817 - val_loss: 0.4519 - val_acc: 0.8501\n",
      "Epoch 14/50\n",
      "6474/6474 [==============================] - 2s 299us/sample - loss: 0.5787 - acc: 0.7851 - val_loss: 0.4566 - val_acc: 0.8553\n",
      "Epoch 15/50\n",
      "6474/6474 [==============================] - 2s 297us/sample - loss: 0.5724 - acc: 0.7898 - val_loss: 0.4509 - val_acc: 0.8582\n",
      "Epoch 16/50\n",
      "6474/6474 [==============================] - 2s 303us/sample - loss: 0.5474 - acc: 0.8017 - val_loss: 0.4275 - val_acc: 0.8663\n",
      "Epoch 17/50\n",
      "6474/6474 [==============================] - 2s 306us/sample - loss: 0.5477 - acc: 0.7955 - val_loss: 0.4081 - val_acc: 0.8681\n",
      "Epoch 18/50\n",
      "6474/6474 [==============================] - 2s 301us/sample - loss: 0.5439 - acc: 0.8044 - val_loss: 0.4181 - val_acc: 0.8686\n",
      "Epoch 19/50\n",
      "6474/6474 [==============================] - 2s 292us/sample - loss: 0.5276 - acc: 0.8046 - val_loss: 0.4036 - val_acc: 0.8698\n",
      "Epoch 20/50\n",
      "6474/6474 [==============================] - 2s 306us/sample - loss: 0.5357 - acc: 0.8065 - val_loss: 0.4111 - val_acc: 0.8686\n",
      "Epoch 21/50\n",
      "6474/6474 [==============================] - 2s 292us/sample - loss: 0.5163 - acc: 0.8157 - val_loss: 0.3967 - val_acc: 0.8767\n",
      "Epoch 22/50\n",
      "6474/6474 [==============================] - 2s 300us/sample - loss: 0.4957 - acc: 0.8222 - val_loss: 0.4016 - val_acc: 0.8779\n",
      "Epoch 23/50\n",
      "6474/6474 [==============================] - 2s 303us/sample - loss: 0.4922 - acc: 0.8225 - val_loss: 0.3981 - val_acc: 0.8779\n",
      "Epoch 24/50\n",
      "6474/6474 [==============================] - 2s 313us/sample - loss: 0.4915 - acc: 0.8236 - val_loss: 0.4001 - val_acc: 0.8721\n",
      "Epoch 25/50\n",
      "6474/6474 [==============================] - 2s 310us/sample - loss: 0.4900 - acc: 0.8248 - val_loss: 0.3782 - val_acc: 0.8848\n",
      "Epoch 26/50\n",
      "6474/6474 [==============================] - 2s 302us/sample - loss: 0.4719 - acc: 0.8318 - val_loss: 0.3675 - val_acc: 0.8744\n",
      "Epoch 27/50\n",
      "6474/6474 [==============================] - 2s 308us/sample - loss: 0.4605 - acc: 0.8395 - val_loss: 0.3851 - val_acc: 0.8767\n",
      "Epoch 28/50\n",
      "6474/6474 [==============================] - 2s 303us/sample - loss: 0.4662 - acc: 0.8316 - val_loss: 0.3822 - val_acc: 0.8796\n",
      "Epoch 29/50\n",
      "6474/6474 [==============================] - 2s 296us/sample - loss: 0.4588 - acc: 0.8367 - val_loss: 0.3687 - val_acc: 0.8831\n",
      "Epoch 30/50\n",
      "6474/6474 [==============================] - 2s 297us/sample - loss: 0.4657 - acc: 0.8340 - val_loss: 0.3517 - val_acc: 0.8819\n",
      "Epoch 31/50\n",
      "6474/6474 [==============================] - 2s 303us/sample - loss: 0.4649 - acc: 0.8321 - val_loss: 0.3550 - val_acc: 0.8872\n",
      "Epoch 32/50\n",
      "6474/6474 [==============================] - 2s 303us/sample - loss: 0.4418 - acc: 0.8378 - val_loss: 0.3519 - val_acc: 0.8860\n",
      "Epoch 33/50\n",
      "6474/6474 [==============================] - 2s 300us/sample - loss: 0.4400 - acc: 0.8400 - val_loss: 0.3437 - val_acc: 0.8883\n",
      "Epoch 34/50\n",
      "6474/6474 [==============================] - 2s 303us/sample - loss: 0.4412 - acc: 0.8415 - val_loss: 0.3758 - val_acc: 0.8843\n",
      "Epoch 35/50\n",
      "6474/6474 [==============================] - 2s 299us/sample - loss: 0.4411 - acc: 0.8446 - val_loss: 0.3749 - val_acc: 0.8848\n",
      "Epoch 36/50\n",
      "6474/6474 [==============================] - 2s 299us/sample - loss: 0.4306 - acc: 0.8435 - val_loss: 0.3507 - val_acc: 0.8877\n",
      "Epoch 37/50\n",
      "6474/6474 [==============================] - 2s 300us/sample - loss: 0.4292 - acc: 0.8477 - val_loss: 0.3386 - val_acc: 0.8854\n",
      "Epoch 38/50\n",
      "6474/6474 [==============================] - 2s 312us/sample - loss: 0.4316 - acc: 0.8414 - val_loss: 0.3541 - val_acc: 0.8935\n",
      "Epoch 39/50\n",
      "6474/6474 [==============================] - 2s 300us/sample - loss: 0.4172 - acc: 0.8502 - val_loss: 0.3390 - val_acc: 0.8854\n",
      "Epoch 40/50\n",
      "6474/6474 [==============================] - 2s 300us/sample - loss: 0.4223 - acc: 0.8503 - val_loss: 0.3269 - val_acc: 0.8877\n",
      "Epoch 41/50\n",
      "6474/6474 [==============================] - 2s 303us/sample - loss: 0.4132 - acc: 0.8553 - val_loss: 0.3520 - val_acc: 0.8877\n",
      "Epoch 42/50\n",
      "6474/6474 [==============================] - 2s 294us/sample - loss: 0.4181 - acc: 0.8508 - val_loss: 0.3224 - val_acc: 0.8941\n",
      "Epoch 43/50\n",
      "6474/6474 [==============================] - 2s 297us/sample - loss: 0.4027 - acc: 0.8590 - val_loss: 0.3348 - val_acc: 0.8935\n",
      "Epoch 44/50\n",
      "6474/6474 [==============================] - 2s 306us/sample - loss: 0.4009 - acc: 0.8570 - val_loss: 0.3235 - val_acc: 0.8947\n",
      "Epoch 45/50\n",
      "6474/6474 [==============================] - 2s 301us/sample - loss: 0.4071 - acc: 0.8551 - val_loss: 0.3349 - val_acc: 0.8958\n",
      "Epoch 46/50\n",
      "6474/6474 [==============================] - 2s 299us/sample - loss: 0.3933 - acc: 0.8585 - val_loss: 0.3298 - val_acc: 0.8918\n",
      "Epoch 47/50\n",
      "6474/6474 [==============================] - 2s 298us/sample - loss: 0.3869 - acc: 0.8618 - val_loss: 0.3434 - val_acc: 0.8906\n",
      "Epoch 48/50\n",
      "6474/6474 [==============================] - 2s 304us/sample - loss: 0.3909 - acc: 0.8607 - val_loss: 0.3271 - val_acc: 0.8958\n",
      "Epoch 49/50\n",
      "6474/6474 [==============================] - 2s 294us/sample - loss: 0.3932 - acc: 0.8624 - val_loss: 0.3256 - val_acc: 0.9005\n",
      "Epoch 50/50\n",
      "6474/6474 [==============================] - 2s 292us/sample - loss: 0.3927 - acc: 0.8622 - val_loss: 0.3740 - val_acc: 0.8854\n",
      " - 0s - loss: 0.8642 - acc: 0.6857\n",
      "Saved trained model at results/final_weights_MLP_tanh_50_epochs_batch_norm.h5 \n",
      "\n",
      "The loss is: 0.86\n",
      "The accuracy is: 0.69\n"
     ]
    }
   ],
   "source": [
    "#Preforming the training by using fit \n",
    "#--------------------------Impelment your code here:-------------------------------------\n",
    "new_a_model.compile(optimizer=AdamOpt, metrics=['accuracy'], loss='categorical_crossentropy')\n",
    "\n",
    "# saving the initial weights\n",
    "if not(\"results\" in os.listdir()):\n",
    "    os.mkdir(\"results\")\n",
    "save_dir = \"results/\"\n",
    "model_name = \"init_weigths_MLP_tanh_task4.h5\"\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "new_a_model.save(model_path)\n",
    "print('Saved initialized model at %s ' % model_path)\n",
    "\n",
    "history = new_a_model.fit(BaseX_train, BaseY_train, batch_size=batch_size, epochs=epochs, validation_split=0, validation_data = (BaseX_val, BaseY_val), shuffle=True)\n",
    "loss_and_metrics = new_a_model.evaluate(X_test, Y_test, verbose=2)\n",
    "\n",
    "# saving the final weights\n",
    "if not(\"results\" in os.listdir()):\n",
    "    os.mkdir(\"results\")\n",
    "save_dir = \"results/\"\n",
    "model_name = \"final_weights_MLP_tanh_50_epochs_batch_norm.h5\"\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "new_a_model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "print()\n",
    "print('The loss is:', np.round(loss_and_metrics[0], 2))\n",
    "print('The accuracy is:', np.round(loss_and_metrics[1], 2))\n",
    "#----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 2: Convolutional Neural Network (CNN)\n",
    "------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:red\">***Task 1:***</span> *2D CNN.* \n",
    "\n",
    "Have a look at the model below and answer the following:\n",
    "\n",
    "**How many layers does it have?**\n",
    "    \n",
    "The model has 8 layers: 5 layers of 2D convolution and 3 layers of fully connected.\n",
    "    \n",
    "    \n",
    "**How many filter in each layer?**\n",
    "    \n",
    "Conv2D_1: 64 filters\n",
    "\n",
    "Conv2D_2: 128 filters\n",
    "\n",
    "Conv2D_3: 128 filters\n",
    "\n",
    "Conv2D_4: 256 filters\n",
    "\n",
    "Conv2D_5: 256 filters\n",
    "    \n",
    "\n",
    "**Would the number of parmaters be similar to a fully connected NN?**\n",
    "    \n",
    "No, the number will be different because in a fully connected NN each neuron is connected to every neuron in the previous layer, and each connection has it's own weight (trainable parameter), while in 2D convolution each neuron is only connected to a few neurons in the previous layer, and the same set of weights is used for every neuron. \n",
    "\n",
    "\n",
    "**Is this specific NN performing regularization?**\n",
    "\n",
    "Yes, the NN performing regularization:\n",
    "* Each 2D convolution layer include regularization kernel (L2).\n",
    "* If drop is True, we have 6 dropouts in the flow. Dropout is a regularization technique that zeros weights of some neurons in the layer randomly (the rate is defined by dropRate).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_net(input_shape,drop,dropRate,reg):\n",
    "    #Defining the network architecture:\n",
    "    model = Sequential()\n",
    "    model.add(Permute((1,2,3),input_shape = input_shape))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu',name='Conv2D_1',kernel_regularizer=regularizers.l2(reg)))\n",
    "    if drop:\n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu',name='Conv2D_2',kernel_regularizer=regularizers.l2(reg)))\n",
    "    if drop:    \n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu',name='Conv2D_3',kernel_regularizer=regularizers.l2(reg)))\n",
    "    if drop:\n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu',name='Conv2D_4',kernel_regularizer=regularizers.l2(reg)))\n",
    "    if drop:\n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu',name='Conv2D_5',kernel_regularizer=regularizers.l2(reg)))\n",
    "    if drop:\n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    #Fully connected network tail:      \n",
    "    model.add(Dense(512, activation='elu',name='FCN_1')) \n",
    "    if drop:\n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(Dense(128, activation='elu',name='FCN_2'))\n",
    "    model.add(Dense(4, activation= 'softmax',name='FCN_3'))\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "permute (Permute)            (None, 32, 32, 1)         0         \n",
      "_________________________________________________________________\n",
      "Conv2D_1 (Conv2D)            (None, 32, 32, 64)        640       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 32, 32, 64)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "Conv2D_2 (Conv2D)            (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 16, 16, 128)       64        \n",
      "_________________________________________________________________\n",
      "Conv2D_3 (Conv2D)            (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 16, 16, 128)       64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "Conv2D_4 (Conv2D)            (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 8, 8, 256)         32        \n",
      "_________________________________________________________________\n",
      "Conv2D_5 (Conv2D)            (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 8, 8, 256)         32        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "FCN_1 (Dense)                (None, 512)               2097664   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "FCN_2 (Dense)                (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "FCN_3 (Dense)                (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 3,271,492\n",
      "Trainable params: 3,271,332\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (32,32,1)\n",
    "learn_rate = 1e-5\n",
    "decay = 1e-03\n",
    "batch_size = 64\n",
    "epochs = 25\n",
    "drop = True\n",
    "dropRate = 0.3\n",
    "reg = 1e-2\n",
    "NNet = get_net(input_shape,drop,dropRate,reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "permute_1 (Permute)          (None, 32, 32, 1)         0         \n",
      "_________________________________________________________________\n",
      "Conv2D_1 (Conv2D)            (None, 32, 32, 64)        640       \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 32, 32, 64)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "Conv2D_2 (Conv2D)            (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 16, 16, 128)       64        \n",
      "_________________________________________________________________\n",
      "Conv2D_3 (Conv2D)            (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 16, 16, 128)       64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "Conv2D_4 (Conv2D)            (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 8, 8, 256)         32        \n",
      "_________________________________________________________________\n",
      "Conv2D_5 (Conv2D)            (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 8, 8, 256)         32        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "FCN_1 (Dense)                (None, 512)               2097664   \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "FCN_2 (Dense)                (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "FCN_3 (Dense)                (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 3,271,492\n",
      "Trainable params: 3,271,332\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NNet=get_net(input_shape,drop,dropRate,reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import *\n",
    "import os\n",
    "from tensorflow.keras.callbacks import *\n",
    "\n",
    "#Defining the optimizar parameters:\n",
    "AdamOpt = Adam(lr=learn_rate,decay=decay)\n",
    "\n",
    "#Compile the network: \n",
    "NNet.compile(optimizer=AdamOpt, metrics=['acc'], loss='categorical_crossentropy')\n",
    "\n",
    "#Saving checkpoints during training:\n",
    "# Checkpath = os.getcwd()\n",
    "# Checkp = ModelCheckpoint(Checkpath, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=True, save_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6474 samples, validate on 1728 samples\n",
      "Epoch 1/25\n",
      "6474/6474 [==============================] - 5s 769us/sample - loss: 8.0198 - acc: 0.4521 - val_loss: 7.8581 - val_acc: 0.2500\n",
      "Epoch 2/25\n",
      "6474/6474 [==============================] - 2s 351us/sample - loss: 7.5996 - acc: 0.5428 - val_loss: 8.0109 - val_acc: 0.2529\n",
      "Epoch 3/25\n",
      "6474/6474 [==============================] - 2s 352us/sample - loss: 7.4377 - acc: 0.5879 - val_loss: 8.0771 - val_acc: 0.2454\n",
      "Epoch 4/25\n",
      "6474/6474 [==============================] - 2s 351us/sample - loss: 7.3416 - acc: 0.6182 - val_loss: 7.9738 - val_acc: 0.2865\n",
      "Epoch 5/25\n",
      "6474/6474 [==============================] - 2s 347us/sample - loss: 7.2284 - acc: 0.6484 - val_loss: 7.8199 - val_acc: 0.2992\n",
      "Epoch 6/25\n",
      "6474/6474 [==============================] - 2s 344us/sample - loss: 7.1604 - acc: 0.6630 - val_loss: 7.6127 - val_acc: 0.3981\n",
      "Epoch 7/25\n",
      "6474/6474 [==============================] - 2s 346us/sample - loss: 7.0866 - acc: 0.6946 - val_loss: 7.5301 - val_acc: 0.4213\n",
      "Epoch 8/25\n",
      "6474/6474 [==============================] - 2s 346us/sample - loss: 7.0238 - acc: 0.7008 - val_loss: 7.4687 - val_acc: 0.4537\n",
      "Epoch 9/25\n",
      "6474/6474 [==============================] - 2s 353us/sample - loss: 6.9805 - acc: 0.7133 - val_loss: 7.4494 - val_acc: 0.4537\n",
      "Epoch 10/25\n",
      "6474/6474 [==============================] - 2s 353us/sample - loss: 6.9343 - acc: 0.7281 - val_loss: 7.4592 - val_acc: 0.4410\n",
      "Epoch 11/25\n",
      "6474/6474 [==============================] - 2s 350us/sample - loss: 6.8901 - acc: 0.7379 - val_loss: 7.4786 - val_acc: 0.4352\n",
      "Epoch 12/25\n",
      "6474/6474 [==============================] - 2s 347us/sample - loss: 6.8397 - acc: 0.7544 - val_loss: 7.4776 - val_acc: 0.4300\n",
      "Epoch 13/25\n",
      "6474/6474 [==============================] - 2s 347us/sample - loss: 6.8051 - acc: 0.7609 - val_loss: 7.4464 - val_acc: 0.4421\n",
      "Epoch 14/25\n",
      "6474/6474 [==============================] - 2s 354us/sample - loss: 6.7766 - acc: 0.7657 - val_loss: 7.4681 - val_acc: 0.4323\n",
      "Epoch 15/25\n",
      "6474/6474 [==============================] - 2s 342us/sample - loss: 6.7499 - acc: 0.7685 - val_loss: 7.4386 - val_acc: 0.4410\n",
      "Epoch 16/25\n",
      "6474/6474 [==============================] - 2s 347us/sample - loss: 6.7225 - acc: 0.7691 - val_loss: 7.4321 - val_acc: 0.4410\n",
      "Epoch 17/25\n",
      "6474/6474 [==============================] - 2s 349us/sample - loss: 6.6874 - acc: 0.7757 - val_loss: 7.4239 - val_acc: 0.4421\n",
      "Epoch 18/25\n",
      "6474/6474 [==============================] - 2s 345us/sample - loss: 6.6580 - acc: 0.7871 - val_loss: 7.4382 - val_acc: 0.4340\n",
      "Epoch 19/25\n",
      "6474/6474 [==============================] - 2s 342us/sample - loss: 6.6322 - acc: 0.7933 - val_loss: 7.4187 - val_acc: 0.4398\n",
      "Epoch 20/25\n",
      "6474/6474 [==============================] - 2s 339us/sample - loss: 6.6073 - acc: 0.8021 - val_loss: 7.4092 - val_acc: 0.4375\n",
      "Epoch 21/25\n",
      "6474/6474 [==============================] - 2s 343us/sample - loss: 6.5741 - acc: 0.8031 - val_loss: 7.3769 - val_acc: 0.4473\n",
      "Epoch 22/25\n",
      "6474/6474 [==============================] - 2s 342us/sample - loss: 6.5627 - acc: 0.8071 - val_loss: 7.3706 - val_acc: 0.4404\n",
      "Epoch 23/25\n",
      "6474/6474 [==============================] - 2s 346us/sample - loss: 6.5350 - acc: 0.8052 - val_loss: 7.3753 - val_acc: 0.4416\n",
      "Epoch 24/25\n",
      "6474/6474 [==============================] - 2s 338us/sample - loss: 6.5171 - acc: 0.8148 - val_loss: 7.3553 - val_acc: 0.4433\n",
      "Epoch 25/25\n",
      "6474/6474 [==============================] - 2s 344us/sample - loss: 6.4851 - acc: 0.8162 - val_loss: 7.3222 - val_acc: 0.4618\n"
     ]
    }
   ],
   "source": [
    "#Preforming the training by using fit \n",
    "# IMPORTANT NOTE: This will take a few minutes!\n",
    "h = NNet.fit(x=BaseX_train, y=BaseY_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0, validation_data = (BaseX_val, BaseY_val), shuffle=True)\n",
    "#NNet.save(model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# NNet.load_weights('Weights_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175/175 [==============================] - 0s 471us/sample - loss: 7.7784 - acc: 0.3200\n",
      "test loss, test acc: [7.7783691024780275, 0.32]\n"
     ]
    }
   ],
   "source": [
    "results = NNet.evaluate(X_test,Y_test)\n",
    "print('test loss, test acc:', results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:red\">***Task 2:***</span> *Number of filters* \n",
    "\n",
    "Rebuild the function `get_net` to have as an input argument a list of number of filters in each layers, i.e. for the CNN defined above the input should have been `[64, 128, 128, 256, 256]`. Now train the model with the number of filters reduced by half. What were the results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "permute_2 (Permute)          (None, 32, 32, 1)         0         \n",
      "_________________________________________________________________\n",
      "Conv2D_1 (Conv2D)            (None, 32, 32, 32)        320       \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "Conv2D_2 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 16, 16, 64)        64        \n",
      "_________________________________________________________________\n",
      "Conv2D_3 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 16, 16, 64)        64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "Conv2D_4 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_15 (B (None, 8, 8, 128)         32        \n",
      "_________________________________________________________________\n",
      "Conv2D_5 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_16 (B (None, 8, 8, 128)         32        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "FCN_1 (Dense)                (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "FCN_2 (Dense)                (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "FCN_3 (Dense)                (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 1,392,772\n",
      "Trainable params: 1,392,612\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Train on 6474 samples, validate on 1728 samples\n",
      "Epoch 1/25\n",
      "6474/6474 [==============================] - 3s 467us/sample - loss: 5.0243 - acc: 0.3699 - val_loss: 4.7269 - val_acc: 0.2500\n",
      "Epoch 2/25\n",
      "6474/6474 [==============================] - 2s 283us/sample - loss: 4.6601 - acc: 0.4666 - val_loss: 4.8498 - val_acc: 0.2500\n",
      "Epoch 3/25\n",
      "6474/6474 [==============================] - 2s 295us/sample - loss: 4.5130 - acc: 0.5002 - val_loss: 4.8971 - val_acc: 0.2500\n",
      "Epoch 4/25\n",
      "6474/6474 [==============================] - 2s 297us/sample - loss: 4.3854 - acc: 0.5443 - val_loss: 4.9069 - val_acc: 0.2500\n",
      "Epoch 5/25\n",
      "6474/6474 [==============================] - 2s 288us/sample - loss: 4.3332 - acc: 0.5550 - val_loss: 4.9278 - val_acc: 0.2500\n",
      "Epoch 6/25\n",
      "6474/6474 [==============================] - 2s 283us/sample - loss: 4.2553 - acc: 0.5778 - val_loss: 4.8637 - val_acc: 0.2454\n",
      "Epoch 7/25\n",
      "6474/6474 [==============================] - 2s 295us/sample - loss: 4.2066 - acc: 0.5958 - val_loss: 4.8045 - val_acc: 0.2535\n",
      "Epoch 8/25\n",
      "6474/6474 [==============================] - 2s 291us/sample - loss: 4.1570 - acc: 0.6066 - val_loss: 4.7413 - val_acc: 0.2708\n",
      "Epoch 9/25\n",
      "6474/6474 [==============================] - 2s 298us/sample - loss: 4.1194 - acc: 0.6236 - val_loss: 4.7246 - val_acc: 0.2755\n",
      "Epoch 10/25\n",
      "6474/6474 [==============================] - 2s 296us/sample - loss: 4.0915 - acc: 0.6350 - val_loss: 4.7242 - val_acc: 0.2784\n",
      "Epoch 11/25\n",
      "6474/6474 [==============================] - 2s 304us/sample - loss: 4.0589 - acc: 0.6386 - val_loss: 4.6986 - val_acc: 0.2836\n",
      "Epoch 12/25\n",
      "5760/6474 [=========================>....] - ETA: 0s - loss: 4.0107 - acc: 0.6571"
     ]
    }
   ],
   "source": [
    "#--------------------------Impelment your code here:-------------------------------------\n",
    "def get_net(filters_list, input_shape, drop, dropRate, reg):\n",
    "    #Defining the network architecture:\n",
    "    model = Sequential()\n",
    "    model.add(Permute((1,2,3),input_shape = input_shape))\n",
    "    model.add(Conv2D(filters=filters_list[0], kernel_size=(3,3), padding='same', activation='relu',name='Conv2D_1',kernel_regularizer=regularizers.l2(reg)))\n",
    "    if drop:\n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(filters=filters_list[1], kernel_size=(3,3), padding='same', activation='relu',name='Conv2D_2',kernel_regularizer=regularizers.l2(reg)))\n",
    "    if drop:    \n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Conv2D(filters=filters_list[2], kernel_size=(3,3), padding='same', activation='relu',name='Conv2D_3',kernel_regularizer=regularizers.l2(reg)))\n",
    "    if drop:\n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(filters=filters_list[3], kernel_size=(3,3), padding='same', activation='relu',name='Conv2D_4',kernel_regularizer=regularizers.l2(reg)))\n",
    "    if drop:\n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Conv2D(filters=filters_list[4], kernel_size=(3,3), padding='same', activation='relu',name='Conv2D_5',kernel_regularizer=regularizers.l2(reg)))\n",
    "    if drop:\n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    #Fully connected network tail:      \n",
    "    model.add(Dense(512, activation='elu',name='FCN_1')) \n",
    "    if drop:\n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(Dense(128, activation='elu',name='FCN_2'))\n",
    "    model.add(Dense(4, activation= 'softmax',name='FCN_3'))\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "input_shape = (32,32,1)\n",
    "learn_rate = 1e-5\n",
    "decay = 1e-03\n",
    "batch_size = 64\n",
    "epochs = 25\n",
    "drop = True\n",
    "dropRate = 0.3\n",
    "reg = 1e-2\n",
    "filters = [32, 64, 64, 128, 128]\n",
    "\n",
    "NNet = get_net(filters, input_shape, drop, dropRate, reg)\n",
    "\n",
    "AdamOpt = Adam(lr=learn_rate,decay=decay)\n",
    "NNet.compile(optimizer=AdamOpt, metrics=['acc'], loss='categorical_crossentropy')\n",
    "\n",
    "history = NNet.fit(x=BaseX_train, y=BaseY_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0, validation_data = (BaseX_val, BaseY_val), shuffle=True)\n",
    "loss_and_metrics = NNet.evaluate(X_test,Y_test)\n",
    "print()\n",
    "print('The loss is:', np.round(loss_and_metrics[0], 2))\n",
    "print('The accuracy is:', np.round(loss_and_metrics[1], 2))\n",
    "\n",
    "#----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all folks! See you :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
