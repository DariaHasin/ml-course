{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2 - â€“ Machine Learning in Healthcare 336546"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Bar Goldner & Daria Hasin*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Accuracy is calculated using this formula: [(TP + TN) / (TP + TN + FP + FN)]. We can see that the accuracy is based on the performance statistics. But as we saw in class, if we have a population of 900 patients of which 100 are AF and 800 are not AF, the accuracy that was calculated is 0.95 which is high, but the data is not balanced as we saw in the confusion matrix, then learning will be poor. Therefore, we prefer to calculate the **performance** statistics and treat them separately rather than relying on model accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. *Definitions*:\n",
    "\n",
    "    **Classifier-1**: uses only Blood Pressure (BP) and Ejection Fraction (EF) features.\n",
    "    \n",
    "    **Classifier-2**: uses all of the features (Age, serum sodium, serum creatinine, gender, smoking, BP, EF, anemia, platelets, Creatinine Phosphokinase (CPK) and diabetes).\n",
    "    \n",
    "    *Comparison between Classifier-1 and Classifier-2*: \n",
    "    #### Classifier-1\n",
    "    ##### Pros: \n",
    "    * Low complexity - less features, less complex model.\n",
    "    * Less patients are needed to train the model.\n",
    "\n",
    "    ##### Cons:\n",
    "    * We dismiss potentially relevant features that we can use for the classification and it can be less accurate.\n",
    "    * Can lead to underfitting and high bias.\n",
    "\n",
    "    #### Classifier-2\n",
    "    ##### Pros: \n",
    "    * We use a lot of features so we don't loose any data about the patients and may have better predictions.\n",
    "\n",
    "    ##### Cons:\n",
    "    * We use all of the features and there is a great probability that not all of them are relevant.\n",
    "    * High complexity and time consuming.\n",
    "    * Can lead to overfitting and high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Two differences between what we will obtain from linear SVM and LR:\n",
    "\n",
    "    1 - The chance of overfitting in the SVM model is lower than in the linear regression model because we have supporting vectors in SVM, that gives the separating line \"safe space\" (margin) from the samples, so it will not be closer to specific class - unlike LR.\n",
    "\n",
    "    2 - The time that takes to run linear SVM is much higher than linear regression, while the results will probably be the same - as Moran mentioned in Tutorial 6. That means that we can obtain our results faster with LR without jeopardizing the quality of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. First, we will define Logistic Regression (LR) and Support Vector Machine (SVM):\n",
    "\n",
    "    **Logistic Regression** is a statistical model that separtes the data to several classes. LR is calculating the probabilities of the patient (for example) to be assighned to a specific class and choosing the class with the *higher probability*. \n",
    "    Note: this is a *linear* model.\n",
    "\n",
    "    **Support Vector Machine** is a model that separtes the data to several classes (like LR),  but SVM takes the *higher distance* between two closest data samples and findes the best hyperplane that separtes the data. Also, in this model there is a use of support vectors that defines the margin to the hyperplane.\n",
    "    Note: this model can be *linear and non-linear*, it dependes on the kernal function (linear / polynominal / rbf) and the dimantion that we are choosing for the calculations. \n",
    "    For example, for kernal=polynominal and d=2 we get a line that separtes the data (linear SVM). \n",
    "\n",
    "    Therefore, linear SVM is based on geometrical properties (distance) of the data while LR relies on statistical preoperties (probabilities).\n",
    "    This difference affect their hyper-parameters tuning, especially on choosing the parameter C. *In SVM, C affects on accuracy*, because it is focused on the trade-off between maximizing the distance between the hyperplane and the support vectors (the margin) and minimizing the misclassified samples.\n",
    "    *In LR, C affects on complexity*, because it is focused on the trade-off between maximizing the number of features in use and simplifying the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold as SKFold\n",
    "import scipy.stats as stats\n",
    "from sklearn.metrics import plot_confusion_matrix, roc_auc_score, confusion_matrix, plot_roc_curve\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00519/heart_failure_clinical_records_dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('heart_failure_clinical_records_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will check if there are nan or non-numeric values in the dataset & replace with random value from the same feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace(regex='[^0-9]', value=np.nan)\n",
    "\n",
    "for feature in df.columns:\n",
    "    one_feature = df.loc[:, feature].tolist()\n",
    "\n",
    "    for idx, x in enumerate(one_feature):\n",
    "        while np.isnan(x):\n",
    "            rand_idx = np.random.choice(len(one_feature))\n",
    "            x = one_feature[rand_idx]\n",
    "            df.loc[idx, feature] = x\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping the feature 'time' because follow-up time is not a relavent feature for heart failure preditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_time = df.copy()\n",
    "df_no_time.drop(columns='time', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will plot an histogram to check outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_no_time.hist(figsize=(15,15), color='purple')\n",
    "x_units = ['Years', 'N.U.', 'mcg/L', 'N.U.', '%', 'N.U.', 'platelets/mL', 'mg/dL', 'mEq/L', 'N.U.', 'N.U.', 'days', 'N.U.']\n",
    "count = 0\n",
    "\n",
    "for i in range(ax.shape[0]):\n",
    "    for j in range(ax.shape[1]):\n",
    "        ax[i][j].set_ylabel('Counts')\n",
    "        try:\n",
    "            ax[i][j].set_xlabel(f'{x_units[count]}')\n",
    "        except:\n",
    "            pass\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No outliers were found.\n",
    "\n",
    "Lastly, we will plot an heatmap to check correlation between features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,8))\n",
    "sns.heatmap(df_no_time.corr(), annot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the highest correlation of the death event feature (by descending order) is with: serum creatinine, ejection fraction, age and serum sodium."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test-train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_no_time.copy()\n",
    "y = X['DEATH_EVENT'].values.ravel()\n",
    "X.drop(columns='DEATH_EVENT', inplace=True) \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization and exploration of the data\n",
    "To show that the distribution of the features is similar between test and train, we chose to plot the distribution of every feature in the two sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5, 2, figsize=(10,20))\n",
    "ax = axs.ravel()\n",
    "features = df.columns\n",
    "x_features = X.columns\n",
    "\n",
    "for idx in range(len(ax)):\n",
    "    ax[idx].hist(X_train.iloc[:, idx], density=True, alpha=0.5, label='train', color='black')\n",
    "    ax[idx].hist(X_test.iloc[:, idx], density=True, alpha=0.5, label='test', color='purple')\n",
    "    ax[idx].legend(loc='upper right')\n",
    "    ax[idx].set_title(features[idx])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we chose to calculate the mean of each feature and compare between the test set and train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_table = pd.DataFrame(index=x_features, columns=['Train', 'Test', 'Delta'])\n",
    "\n",
    "for feat in x_features:\n",
    "    feat_mean_train = np.round(X_train.loc[:, feat].mean(), 2)\n",
    "    d_table.loc[feat, 'Train'] = feat_mean_train\n",
    "    \n",
    "    feat_mean_test = np.round(X_test.loc[:, feat].mean(), 2)\n",
    "    d_table.loc[feat, 'Test'] = feat_mean_test\n",
    "    \n",
    "    d_table.loc[feat, 'Delta'] = abs(feat_mean_train-feat_mean_test)\n",
    "\n",
    "feat = 'DEATH_EVENT'\n",
    "feat_mean_y_train = np.round(np.mean(y_train), 2)\n",
    "d_table.loc[feat, 'Train'] = feat_mean_y_train\n",
    "\n",
    "feat_mean_y_test = np.round(np.mean(y_test), 2)\n",
    "d_table.loc[feat, 'Test'] = feat_mean_y_test\n",
    "\n",
    "d_table.loc[feat, 'Delta'] = abs(feat_mean_y_train-feat_mean_y_test)\n",
    "\n",
    "d_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What issues could imbalance the features between train and test cause? \n",
    "If the train's features distribution is different than the test's features distribution, the model can calculate the prediction in a wrong way. The model calculation is based on the learning from the train data set, and it applies the learning on the test data set, so if there are different distribution, the applying part will be wrong.\n",
    "\n",
    "#### How could you solve the issue?\n",
    "We can use the \"stratify\" attribute that in the \" train_test_split\" function which make the data split in a stratified fashion, using the class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots that show the relationship between feature and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 3, figsize=(15,10))\n",
    "ax = axs.ravel()\n",
    "boolian_features = ['anaemia', 'high_blood_pressure', 'diabetes', 'sex' ,'smoking']\n",
    "check_feat = 'DEATH_EVENT'\n",
    "\n",
    "for idx, feat in enumerate(boolian_features):\n",
    "    countplot = sns.countplot(ax=ax[idx] ,x=feat, data=df[[feat, check_feat]], hue=check_feat, palette='Set2')\n",
    "    if feat == 'sex':\n",
    "        countplot.set_xticklabels(['Women', 'Men'])\n",
    "    else:\n",
    "        countplot.set_xticklabels(['No', 'Yes'])\n",
    "    \n",
    "    countplot.legend(labels=['No', 'Yes'], title='Death event')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Additional plots to see correlation between binary and non-binary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x=\"age\", y=\"high_blood_pressure\", hue=\"DEATH_EVENT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x=\"creatinine_phosphokinase\", y=\"diabetes\", hue=\"DEATH_EVENT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x=\"ejection_fraction\", y=\"high_blood_pressure\", hue=\"DEATH_EVENT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Was there anything unexpected?\n",
    "As we can see on the plots that show the relationship between feature and label, most of the binary feature divided is similar in terms of number of deaths. It was unexpected to find out that there is not linear correlation of death between the two features \"age\" and \"high_blood_pressure\" and also between the two features \"age\" and \"diabetes\".\n",
    "\n",
    "#### Are there any features that you feel will be particularly important to your model? Explain why.\n",
    "When we went through the dataset, we noticed that the smaller the value of the feature \"time\", the higher the deaths. When we thought about the meaning of the feature, we realized that this feature represents the patients' follow-up time, so it makes sense. After much thought we decided to remove this feature from the dataset even though we know it will hurt the chances of predicting correctly. That's because this feature does not really represent a meaning to any person who has not taken part of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Models\n",
    "### Linear model - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_penalty(penalty='none'):\n",
    "    if penalty == 'l1':\n",
    "        solver='liblinear'\n",
    "    if penalty == 'l2' or penalty == 'none':\n",
    "        solver='lbfgs'\n",
    "    return solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "C = np.array([0.001, 0.01, 1, 10, 100, 1000])\n",
    "K = 5 \n",
    "penalty = ['l1', 'l2']\n",
    "dict_values = list()\n",
    "\n",
    "kf = SKFold(n_splits=K, random_state=10, shuffle=True)\n",
    "\n",
    "with tqdm(total=len(C), file=sys.stdout) as pbar:\n",
    "    for c in C:\n",
    "        pbar.set_description('processed: %d/%d' % ((1 + idx), len(C)))\n",
    "        pbar.update(1)\n",
    "        for p in penalty:\n",
    "            solver = check_penalty(p)\n",
    "            logreg = LogisticRegression(solver=solver, penalty=p, C=c, max_iter=10000, multi_class='ovr')\n",
    "            loss_val_vec = np.zeros(K)\n",
    "            k = 0\n",
    "            for train_idx, val_idx in kf.split(X_train, y_train):\n",
    "                x_train_tmp, x_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "                y_train_tmp, y_val = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "                x_train_tmp = scaler.fit_transform(x_train_tmp)\n",
    "                x_val = scaler.transform(x_val)\n",
    "\n",
    "                logreg.fit(x_train_tmp, y_train_tmp)\n",
    "                y_pred_val = logreg.predict_proba(x_val)\n",
    "                loss_val_vec[k] = log_loss(y_val, y_pred_val)\n",
    "                k += 1\n",
    "            dict_values.append({'C': c, 'penalty': p, 'mu': np.mean(loss_val_vec), 'sigma': np.std(loss_val_vec)})\n",
    "\n",
    "loss_arr = [x['mu'] for x in dict_values]\n",
    "argMinLoss = np.argmin(loss_arr)\n",
    "best_c = dict_values[argMinLoss]['C']\n",
    "best_penalty = dict_values[argMinLoss]['penalty']\n",
    "best_loss = dict_values[argMinLoss]['mu']\n",
    "print(f'The chosen C: {best_c} and the chosen penalty: {best_penalty}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in dict_values:\n",
    "    x = np.linspace(0, d['mu'] + 3 * d['sigma'], 1000)\n",
    "    plt.plot(x, stats.norm.pdf(x, d['mu'], d['sigma']), label=\"p = \" + d['penalty'] + \", C = \" + str(d['C'])) \n",
    "    plt.title('Gaussian distribution of the loss')\n",
    "    plt.xlabel('Average loss')\n",
    "    plt.ylabel('Probabilty density')\n",
    "    plt.ylim(0, 70)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = check_penalty(best_penalty)\n",
    "logreg = LogisticRegression(solver=solver, penalty=best_penalty, C=best_c, max_iter=10000, multi_class='ovr')\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "plot_confusion_matrix(logreg, X_test, y_test, cmap=plt.cm.Blues)\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_TN = lambda y_true, y_pred: confusion_matrix(y_true, y_pred)[0, 0]\n",
    "calc_FP = lambda y_true, y_pred: confusion_matrix(y_true, y_pred)[0, 1]\n",
    "calc_FN = lambda y_true, y_pred: confusion_matrix(y_true, y_pred)[1, 0]\n",
    "calc_TP = lambda y_true, y_pred: confusion_matrix(y_true, y_pred)[1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_stats(y_test, y_pred_test):\n",
    "    TN = calc_TN(y_test, y_pred_test)\n",
    "    FP = calc_FP(y_test, y_pred_test)\n",
    "    FN = calc_FN(y_test, y_pred_test)\n",
    "    TP = calc_TP(y_test, y_pred_test)\n",
    "    Se = TP/(TP+FN)\n",
    "    Sp = TN/(TN+FP)\n",
    "    PPV = TP/(TP+FP)\n",
    "    NPV = TN/(TN+FN)\n",
    "    Acc = (TP+TN)/(TP+TN+FP+FN)\n",
    "    F1 = (2*Se*PPV)/(Se+PPV)\n",
    "    return TN, FP, FN, TP, Se, Sp, PPV, NPV, Acc, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = logreg.predict(X_test)\n",
    "y_pred_proba_test = logreg.predict_proba(X_test)\n",
    "\n",
    "\n",
    "TN, FP, FN, TP, Se, Sp, PPV, NPV, Acc, F1 = check_stats(y_test, y_pred_test)\n",
    "print('Sensitivity is {:.4f}. \\nSpecificity is {:.4f}. \\nPPV is {:.4f}. \\nNPV is {:.4f}. \\nAccuracy is {:.4f}. \\nF1 is {:.4f}. '.format(Se,Sp,PPV,NPV,Acc,F1))\n",
    "mcc = ((TP * TN) - (FP * FN)) / np.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))\n",
    "print('MCC is {:.4f}.'.format(mcc))\n",
    "print('AUROC is {:.4f}.'.format(roc_auc_score(y_test, y_pred_test)))\n",
    "print('Loss is: {:.4f}.'.format(best_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(logreg, X_test, y_test)\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-linear model - Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(probability=True, random_state=10, max_iter=1000)\n",
    "pipe = Pipeline(steps=[('scale', MinMaxScaler()), ('svm', svc)])\n",
    "\n",
    "svm_nonlin = GridSearchCV(estimator=pipe,\n",
    "             param_grid={'svm__kernel':['rbf', 'poly'], 'svm__C':C, 'svm__degree':[2, 3, 4]},\n",
    "             scoring='roc_auc', \n",
    "             cv=kf, refit='f1', verbose=3, return_train_score=True)\n",
    "svm_nonlin.fit(X_train, y_train)\n",
    "best_svm_nonlin = svm_nonlin.best_estimator_\n",
    "print()\n",
    "print('Best parameters for SVM:', svm_nonlin.best_params_)\n",
    "\n",
    "y_pred_test = best_svm_nonlin.predict(X_test)\n",
    "y_pred_proba_test = best_svm_nonlin.predict_proba(X_test)\n",
    "plot_confusion_matrix(best_svm_nonlin, X_test, y_test, cmap=plt.cm.Blues)\n",
    "plt.grid(False)\n",
    "\n",
    "TN, FP, FN, TP, Se, Sp, PPV, NPV, Acc, F1 = check_stats(y_test, y_pred_test)\n",
    "print()\n",
    "print('Sensitivity is {:.4f}. \\nSpecificity is {:.4f}. \\nPPV is {:.2f}. \\nNPV is {:.4f}. \\nAccuracy is {:.4f}. \\nF1 is {:.4f}. '.format(Se,Sp,PPV,NPV,Acc,F1))\n",
    "mcc = ((TP * TN) - (FP * FN)) / np.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))\n",
    "print('MCC is {:.4f}.'.format(mcc))\n",
    "print('AUROC is {:.4f}.'.format(roc_auc_score(y_test, y_pred_proba_test[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(svm_nonlin, X_test, y_test)\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What performs best on this dataset? Linear or non-linear models?\n",
    "**LR:**\n",
    "\n",
    "*Accuracy:* 0.8000. \n",
    "\n",
    "*F1:* 0.6471.\n",
    "\n",
    "*AUROC:* 0.7407.\n",
    "\n",
    "**SVM:**\n",
    "\n",
    "*Accuracy:* 0.6667.\n",
    "\n",
    "*F1:* 0.4444. \n",
    "\n",
    "*AUROC:* 0.7189.\n",
    "\n",
    "We can see that the *linear model* performed better than the non-linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(random_state=10)\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "y_pred_test = rfc.predict(X_test) #NOTICE NOT TO USE THE STANDARDIZED DATA.\n",
    "y_pred_proba_test = rfc.predict_proba(X_test)\n",
    "plot_confusion_matrix(rfc,np.array(X_test),y_test, cmap=plt.cm.Blues)\n",
    "plt.grid(False)\n",
    "TN, FP, FN, TP, Se, Sp, PPV, NPV, Acc, F1 = check_stats(y_test, y_pred_test)\n",
    "print('Sensitivity is {:.4f}. \\nSpecificity is {:.4f}. \\nPPV is {:.2f}. \\nNPV is {:.4f}. \\nAccuracy is {:.4f}. \\nF1 is {:.4f}. '.format(Se,Sp,PPV,NPV,Acc,F1))\n",
    "mcc = ((TP * TN) - (FP * FN)) / np.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))\n",
    "print('MCC is {:.4f}.'.format(mcc))\n",
    "print('AUROC is {:.4f}.'.format(roc_auc_score(y_test, y_pred_proba_test[:,1])))\n",
    "\n",
    "roc_score = []\n",
    "plt.figure()\n",
    "ax = plt.gca()\n",
    "plot_roc_curve(rfc, X_test, y_test, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the accuracy and the f1 in the Random Forast model is lower than the accuracy of LR model, but AUROC is higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rfc.feature_importances_\n",
    "forest_importances = pd.Series(importances, index=x_features)\n",
    "std = np.std([tree.feature_importances_ for tree in rfc.estimators_], axis=0)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(yerr=std, ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the 2 most important features according to the random forest.\n",
    "The 2 most important features are serum creatinine and ejection fraction.\n",
    "\n",
    "#### Does this match up exactly with the feature exploration you did?\n",
    "It is match up  with the feature exploration becuse when we checked the correlation both of those features showed the highest values in absolute value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Separability Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 2\n",
    "pca = PCA(n_components=n_components, whiten=True)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "Z_train = pca.fit_transform(X_train_scaled)\n",
    "Z_test = pca.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_2d_pca(X_pca,y):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(111, aspect='equal')\n",
    "    ax.scatter(X_pca[y==0, 0], X_pca[y==0, 1], color='b')\n",
    "    ax.scatter(X_pca[y==1, 0], X_pca[y==1, 1], color='r')\n",
    "    ax.legend(('Alive','Dead'))\n",
    "    ax.plot([0], [0], \"ko\")\n",
    "    ax.arrow(0, 0, 0, 1, head_width=0.05, length_includes_head=True, head_length=0.1, fc='k', ec='k')\n",
    "    ax.arrow(0, 0, 1, 0, head_width=0.05, length_includes_head=True, head_length=0.1, fc='k', ec='k')\n",
    "    ax.set_xlabel('$U_1$')\n",
    "    ax.set_ylabel('$U_2$')\n",
    "    ax.set_title('2D PCA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_2d_pca(Z_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How separable is your data when reduced to just two features?\n",
    "The data is not separable because the 'Dead' and 'Alive' classes are mixed together and do not create typical clusters, as an opposite to our expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_values = list()\n",
    "\n",
    "with tqdm(total=len(C), file=sys.stdout) as pbar:\n",
    "    for c in C:\n",
    "        pbar.set_description('processed: %d/%d' % ((1 + idx), len(C)))\n",
    "        pbar.update(1)\n",
    "        for p in penalty:\n",
    "            solver = check_penalty(p)\n",
    "            logreg = LogisticRegression(solver=solver, penalty=p, C=c, max_iter=10000, multi_class='ovr')\n",
    "            loss_val_vec = np.zeros(K)\n",
    "            k = 0\n",
    "            for train_idx, val_idx in kf.split(Z_train, y_train):\n",
    "                z_train_tmp, z_val = Z_train[train_idx], Z_train[val_idx]\n",
    "                y_train_tmp, y_val = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "                z_train_tmp = scaler.fit_transform(z_train_tmp)\n",
    "                z_val = scaler.transform(z_val)\n",
    "\n",
    "                logreg.fit(z_train_tmp, y_train_tmp)\n",
    "                y_pred_val = logreg.predict_proba(z_val)\n",
    "                loss_val_vec[k] = log_loss(y_val, y_pred_val)\n",
    "                k += 1\n",
    "            dict_values.append({'C': c, 'penalty': p, 'mu': np.mean(loss_val_vec), 'sigma': np.std(loss_val_vec)})\n",
    "\n",
    "loss_arr = [x['mu'] for x in dict_values]\n",
    "argMinLoss = np.argmin(loss_arr)\n",
    "best_c = dict_values[argMinLoss]['C']\n",
    "best_penalty = dict_values[argMinLoss]['penalty']\n",
    "best_loss = dict_values[argMinLoss]['mu']\n",
    "print()\n",
    "print(f'The chosen C: {best_c} and the chosen penalty: {best_penalty}')\n",
    "solver = check_penalty(best_penalty)\n",
    "logreg = LogisticRegression(solver=solver, penalty=best_penalty, C=best_c, max_iter=10000, multi_class='ovr')\n",
    "logreg.fit(Z_train, y_train)\n",
    "\n",
    "plot_confusion_matrix(logreg, Z_test, y_test, cmap=plt.cm.Blues)\n",
    "plt.grid(False)\n",
    "\n",
    "TN, FP, FN, TP, Se, Sp, PPV, NPV, Acc, F1 = check_stats(y_test, y_pred_test)\n",
    "print('Sensitivity is {:.4f}. \\nSpecificity is {:.4f}. \\nPPV is {:.2f}. \\nNPV is {:.4f}. \\nAccuracy is {:.4f}. \\nF1 is {:.4f}. '.format(Se,Sp,PPV,NPV,Acc,F1))\n",
    "mcc = ((TP * TN) - (FP * FN)) / np.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))\n",
    "print('MCC is {:.4f}.'.format(mcc))\n",
    "print('AUROC is {:.4f}.'.format(roc_auc_score(y_test, y_pred_proba_test[:,1])))\n",
    "print('Loss is: {:.4f}.'.format(best_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_nonlin.fit(Z_train, y_train)\n",
    "best_svm_nonlin = svm_nonlin.best_estimator_\n",
    "print()\n",
    "print('Best parameters for SVM:', svm_nonlin.best_params_)\n",
    "\n",
    "y_pred_test = best_svm_nonlin.predict(Z_test)\n",
    "y_pred_proba_test = best_svm_nonlin.predict_proba(Z_test)\n",
    "plot_confusion_matrix(best_svm_nonlin, Z_test, y_test, cmap=plt.cm.Blues)\n",
    "plt.grid(False)\n",
    "\n",
    "TN, FP, FN, TP, Se, Sp, PPV, NPV, Acc, F1 = check_stats(y_test, y_pred_test)\n",
    "print()\n",
    "print('Sensitivity is {:.4f}. \\nSpecificity is {:.4f}. \\nPPV is {:.2f}. \\nNPV is {:.4f}. \\nAccuracy is {:.4f}. \\nF1 is {:.4f}. '.format(Se,Sp,PPV,NPV,Acc,F1))\n",
    "mcc = ((TP * TN) - (FP * FN)) / np.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))\n",
    "print('MCC is {:.4f}.'.format(mcc))\n",
    "print('AUROC is {:.4f}.'.format(roc_auc_score(y_test, y_pred_proba_test[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_values = list()\n",
    "\n",
    "two_feat_X_train = X_train.loc[:,{'serum_creatinine','ejection_fraction'}]\n",
    "two_feat_X_test = X_test.loc[:,{'serum_creatinine','ejection_fraction'}]\n",
    "\n",
    "kf = SKFold(n_splits=K, random_state=10, shuffle=True)\n",
    "\n",
    "with tqdm(total=len(C), file=sys.stdout) as pbar:\n",
    "    for c in C:\n",
    "        pbar.set_description('processed: %d/%d' % ((1 + idx), len(C)))\n",
    "        pbar.update(1)\n",
    "        for p in penalty:\n",
    "            solver = check_penalty(p)\n",
    "            logreg = LogisticRegression(solver=solver, penalty=p, C=c, max_iter=10000, multi_class='ovr')\n",
    "            loss_val_vec = np.zeros(K)\n",
    "            k = 0\n",
    "            for train_idx, val_idx in kf.split(two_feat_X_train, y_train):\n",
    "                x_train_tmp, x_val = two_feat_X_train.iloc[train_idx], two_feat_X_train.iloc[val_idx]\n",
    "                y_train_tmp, y_val = y_train[train_idx], y_train[val_idx]\n",
    "                x_train_tmp = scaler.fit_transform(x_train_tmp)\n",
    "                x_val = scaler.transform(x_val)\n",
    "\n",
    "                logreg.fit(x_train_tmp, y_train_tmp)\n",
    "                y_pred_val = logreg.predict_proba(x_val)\n",
    "                loss_val_vec[k] = log_loss(y_val, y_pred_val)\n",
    "                k += 1\n",
    "            dict_values.append({'C': c, 'penalty': p, 'mu': np.mean(loss_val_vec), 'sigma': np.std(loss_val_vec)})\n",
    "\n",
    "loss_arr = [x['mu'] for x in dict_values]\n",
    "argMinLoss = np.argmin(loss_arr)\n",
    "best_c = dict_values[argMinLoss]['C']\n",
    "best_penalty = dict_values[argMinLoss]['penalty']\n",
    "best_loss = dict_values[argMinLoss]['mu']\n",
    "print()\n",
    "print(f'The chosen C: {best_c} and the chosen penalty: {best_penalty}')\n",
    "\n",
    "solver = check_penalty(best_penalty)\n",
    "logreg = LogisticRegression(solver=solver, penalty=best_penalty, C=best_c, max_iter=10000, multi_class='ovr')\n",
    "logreg.fit(two_feat_X_train, y_train)\n",
    "\n",
    "plot_confusion_matrix(logreg, two_feat_X_test, y_test, cmap=plt.cm.Blues)\n",
    "plt.grid(False)\n",
    "\n",
    "y_pred_test = logreg.predict(two_feat_X_test)\n",
    "y_pred_proba_test = logreg.predict_proba(two_feat_X_test)\n",
    "\n",
    "\n",
    "TN, FP, FN, TP, Se, Sp, PPV, NPV, Acc, F1 = check_stats(y_test, y_pred_test)\n",
    "print('Sensitivity is {:.4f}. \\nSpecificity is {:.4f}. \\nPPV is {:.4f}. \\nNPV is {:.4f}. \\nAccuracy is {:.4f}. \\nF1 is {:.4f}. '.format(Se,Sp,PPV,NPV,Acc,F1))\n",
    "mcc = ((TP * TN) - (FP * FN)) / np.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))\n",
    "print('MCC is {:.4f}.'.format(mcc))\n",
    "print('AUROC is {:.4f}.'.format(roc_auc_score(y_test, y_pred_test)))\n",
    "print('Loss is: {:.4f}.'.format(best_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_nonlin.fit(two_feat_X_train, y_train)\n",
    "best_svm_nonlin = svm_nonlin.best_estimator_\n",
    "print()\n",
    "print('Best parameters for SVM:', svm_nonlin.best_params_)\n",
    "\n",
    "y_pred_test = best_svm_nonlin.predict(two_feat_X_test)\n",
    "y_pred_proba_test = best_svm_nonlin.predict_proba(two_feat_X_test)\n",
    "plot_confusion_matrix(best_svm_nonlin, two_feat_X_test, y_test, cmap=plt.cm.Blues)\n",
    "plt.grid(False)\n",
    "\n",
    "TN, FP, FN, TP, Se, Sp, PPV, NPV, Acc, F1 = check_stats(y_test, y_pred_test)\n",
    "print('Sensitivity is {:.4f}. \\nSpecificity is {:.4f}. \\nPPV is {:.2f}. \\nNPV is {:.4f}. \\nAccuracy is {:.4f}. \\nF1 is {:.4f}. '.format(Se,Sp,PPV,NPV,Acc,F1))\n",
    "mcc = ((TP * TN) - (FP * FN)) / np.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))\n",
    "print('MCC is {:.4f}.'.format(mcc))\n",
    "print('AUROC is {:.4f}.'.format(roc_auc_score(y_test, y_pred_proba_test[:,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What performs better? 2 features or the reduced dimensionality.\n",
    "\n",
    "#### reduced dimensionality performance statistics:\n",
    "\n",
    "**LR:**\n",
    "\n",
    "*Accuracy:* 0.7833. \n",
    "\n",
    "*F1:* 0.6061.\n",
    "\n",
    "*AUROC:* 0.8299.\n",
    "\n",
    "**SVM:**\n",
    "\n",
    "*Accuracy:* 0.7167.\n",
    "\n",
    "*F1:* 0.1905. \n",
    "\n",
    "*AUROC:* 0.7150.\n",
    "\n",
    "\n",
    "#### 2 feature performance statistics:\n",
    "\n",
    "**LR:**\n",
    "\n",
    "*Accuracy:* 0.7333. \n",
    "\n",
    "*F1:* 0.4667.\n",
    "\n",
    "*AUROC:* 0.6354.\n",
    "\n",
    "**SVM:**\n",
    "\n",
    "*Accuracy:* 0.7333. \n",
    "\n",
    "*F1:* 0.5294. \n",
    "\n",
    "*AUROC:* 0.8151.\n",
    "\n",
    "\n",
    "We can see that the *dimensionality reduced LR model* preformed best. Also, we see that the 2 features SVM preformed better than dimensionality reduced SVM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### That's all :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
